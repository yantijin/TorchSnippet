

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>TorchSnippet.Layers package &mdash; TorchSnippet  documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="TorchSnippet.ode package" href="TorchSnippet.ode.html" />
    <link rel="prev" title="TorchSnippet.Flow package" href="TorchSnippet.Flow.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> TorchSnippet
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">TorchSnippet</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="TorchSnippet.html">TorchSnippet package</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="TorchSnippet.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="TorchSnippet.Flow.html">TorchSnippet.Flow package</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">TorchSnippet.Layers package</a></li>
<li class="toctree-l4"><a class="reference internal" href="TorchSnippet.ode.html">TorchSnippet.ode package</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="TorchSnippet.html#submodules">Submodules</a></li>
<li class="toctree-l3"><a class="reference internal" href="TorchSnippet.html#module-TorchSnippet.arg_check">TorchSnippet.arg_check module</a></li>
<li class="toctree-l3"><a class="reference internal" href="TorchSnippet.html#module-TorchSnippet.core">TorchSnippet.core module</a></li>
<li class="toctree-l3"><a class="reference internal" href="TorchSnippet.html#module-TorchSnippet.typing_">TorchSnippet.typing_ module</a></li>
<li class="toctree-l3"><a class="reference internal" href="TorchSnippet.html#module-TorchSnippet">Module contents</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">TorchSnippet.Layers package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-TorchSnippet.Layers.activation">TorchSnippet.Layers.activation module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-TorchSnippet.Layers.base">TorchSnippet.Layers.base module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-TorchSnippet.Layers.gated">TorchSnippet.Layers.gated module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-TorchSnippet.Layers.initializer">TorchSnippet.Layers.initializer module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-TorchSnippet.Layers.layers">TorchSnippet.Layers.layers module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-TorchSnippet.Layers.utils">TorchSnippet.Layers.utils module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-TorchSnippet.Layers">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="TorchSnippet.Flow.html">TorchSnippet.Flow package</a></li>
<li class="toctree-l1"><a class="reference internal" href="TorchSnippet.ode.html">TorchSnippet.ode package</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">TorchSnippet</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
          <li><a href="modules.html">TorchSnippet</a> &raquo;</li>
        
          <li><a href="TorchSnippet.html">TorchSnippet package</a> &raquo;</li>
        
      <li>TorchSnippet.Layers package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/TorchSnippet.Layers.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="torchsnippet-layers-package">
<h1>TorchSnippet.Layers package<a class="headerlink" href="#torchsnippet-layers-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-TorchSnippet.Layers.activation">
<span id="torchsnippet-layers-activation-module"></span><h2>TorchSnippet.Layers.activation module<a class="headerlink" href="#module-TorchSnippet.Layers.activation" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="TorchSnippet.Layers.activation.ReLU">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.activation.</code><code class="sig-name descname">ReLU</code><a class="headerlink" href="#TorchSnippet.Layers.activation.ReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#TorchSnippet.Layers.base.BaseSingleVariateLayer" title="TorchSnippet.Layers.base.BaseSingleVariateLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TorchSnippet.Layers.base.BaseSingleVariateLayer</span></code></a></p>
</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.activation.LeakyReLU">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.activation.</code><code class="sig-name descname">LeakyReLU</code><span class="sig-paren">(</span><em class="sig-param">negative_slope=0.01</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.activation.LeakyReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#TorchSnippet.Layers.base.BaseSingleVariateLayer" title="TorchSnippet.Layers.base.BaseSingleVariateLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TorchSnippet.Layers.base.BaseSingleVariateLayer</span></code></a></p>
<dl class="attribute">
<dt id="TorchSnippet.Layers.activation.LeakyReLU.negative_slope">
<code class="sig-name descname">negative_slope</code><em class="property">: float</em><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.activation.LeakyReLU.negative_slope" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.activation.Tanh">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.activation.</code><code class="sig-name descname">Tanh</code><a class="headerlink" href="#TorchSnippet.Layers.activation.Tanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#TorchSnippet.Layers.base.BaseSingleVariateLayer" title="TorchSnippet.Layers.base.BaseSingleVariateLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TorchSnippet.Layers.base.BaseSingleVariateLayer</span></code></a></p>
</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.activation.Sigmoid">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.activation.</code><code class="sig-name descname">Sigmoid</code><a class="headerlink" href="#TorchSnippet.Layers.activation.Sigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#TorchSnippet.Layers.base.BaseSingleVariateLayer" title="TorchSnippet.Layers.base.BaseSingleVariateLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TorchSnippet.Layers.base.BaseSingleVariateLayer</span></code></a></p>
</dd></dl>

</div>
<div class="section" id="module-TorchSnippet.Layers.base">
<span id="torchsnippet-layers-base-module"></span><h2>TorchSnippet.Layers.base module<a class="headerlink" href="#module-TorchSnippet.Layers.base" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="TorchSnippet.Layers.base.DEFAULT_BIAS_INIT">
<code class="sig-prename descclassname">TorchSnippet.Layers.base.</code><code class="sig-name descname">DEFAULT_BIAS_INIT</code><span class="sig-paren">(</span><em class="sig-param">tensor: torch.Tensor</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.base.DEFAULT_BIAS_INIT" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.Layers.base.DEFAULT_WEIGHT_INIT">
<code class="sig-prename descclassname">TorchSnippet.Layers.base.</code><code class="sig-name descname">DEFAULT_WEIGHT_INIT</code><span class="sig-paren">(</span><em class="sig-param">tensor: torch.Tensor</em>, <em class="sig-param">gain: float = 1.0</em>, <em class="sig-param">fan_in_and_fan_out: Optional[Tuple[int</em>, <em class="sig-param">int]] = None</em>, <em class="sig-param">mode: str = 'fan_in'</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.base.DEFAULT_WEIGHT_INIT" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.base.BaseParamStore">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.base.</code><code class="sig-name descname">BaseParamStore</code><span class="sig-paren">(</span><em class="sig-param">shape: List[int]</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.base.BaseParamStore" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Base class for a component that stores a trainable parameter,
or a set of trainable parameters that can be used to derive
virtually “a parameter” (e.g., weight-normed <cite>weight</cite>).</p>
<dl class="method">
<dt id="TorchSnippet.Layers.base.BaseParamStore.extra_repr">
<code class="sig-name descname">extra_repr</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; str<a class="headerlink" href="#TorchSnippet.Layers.base.BaseParamStore.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should reimplement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.Layers.base.BaseParamStore.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#TorchSnippet.Layers.base.BaseParamStore.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.Layers.base.BaseParamStore.get">
<code class="sig-name descname">get</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#TorchSnippet.Layers.base.BaseParamStore.get" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="TorchSnippet.Layers.base.BaseParamStore.set">
<code class="sig-name descname">set</code><span class="sig-paren">(</span><em class="sig-param">value: Union[Tensor, numpy.ndarray, float, int]</em><span class="sig-paren">)</span> &#x2192; None<a class="headerlink" href="#TorchSnippet.Layers.base.BaseParamStore.set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.BaseParamStore.shape">
<code class="sig-name descname">shape</code><em class="property">: List[int]</em><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.BaseParamStore.shape" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.base.SimpleParamStore">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.base.</code><code class="sig-name descname">SimpleParamStore</code><span class="sig-paren">(</span><em class="sig-param">shape: List[int], initializer: Union[int, float, numpy.ndarray, Callable[[...], None]]</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.base.SimpleParamStore" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#TorchSnippet.Layers.base.BaseParamStore" title="TorchSnippet.Layers.base.BaseParamStore"><code class="xref py py-class docutils literal notranslate"><span class="pre">TorchSnippet.Layers.base.BaseParamStore</span></code></a></p>
<p>A module that carries a direct variable as the parameter.</p>
<dl class="method">
<dt id="TorchSnippet.Layers.base.SimpleParamStore.get">
<code class="sig-name descname">get</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#TorchSnippet.Layers.base.SimpleParamStore.get" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="TorchSnippet.Layers.base.SimpleParamStore.set">
<code class="sig-name descname">set</code><span class="sig-paren">(</span><em class="sig-param">value: Union[Tensor, numpy.ndarray, float, int]</em><span class="sig-paren">)</span> &#x2192; None<a class="headerlink" href="#TorchSnippet.Layers.base.SimpleParamStore.set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.SimpleParamStore.shape">
<code class="sig-name descname">shape</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.SimpleParamStore.shape" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.base.NormedWeightStore">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.base.</code><code class="sig-name descname">NormedWeightStore</code><span class="sig-paren">(</span><em class="sig-param">shape: List[int], initializer: Union[int, float, numpy.ndarray, Callable[[...], None]], norm_axis: int = 1, epsilon: float = 1e-05</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.base.NormedWeightStore" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#TorchSnippet.Layers.base.BaseParamStore" title="TorchSnippet.Layers.base.BaseParamStore"><code class="xref py py-class docutils literal notranslate"><span class="pre">TorchSnippet.Layers.base.BaseParamStore</span></code></a></p>
<p>A module that carries the weight-normed <cite>weight</cite>, without <cite>g</cite>.</p>
<dl class="attribute">
<dt id="TorchSnippet.Layers.base.NormedWeightStore.epsilon">
<code class="sig-name descname">epsilon</code><em class="property">: float</em><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.NormedWeightStore.epsilon" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="TorchSnippet.Layers.base.NormedWeightStore.get">
<code class="sig-name descname">get</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#TorchSnippet.Layers.base.NormedWeightStore.get" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.NormedWeightStore.norm_axis">
<code class="sig-name descname">norm_axis</code><em class="property">: int</em><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.NormedWeightStore.norm_axis" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="TorchSnippet.Layers.base.NormedWeightStore.set">
<code class="sig-name descname">set</code><span class="sig-paren">(</span><em class="sig-param">value: Union[Tensor, numpy.ndarray, float, int]</em><span class="sig-paren">)</span> &#x2192; None<a class="headerlink" href="#TorchSnippet.Layers.base.NormedWeightStore.set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="TorchSnippet.Layers.base.weight_norm_decompose">
<code class="sig-prename descclassname">TorchSnippet.Layers.base.</code><code class="sig-name descname">weight_norm_decompose</code><span class="sig-paren">(</span><em class="sig-param">weight: torch.Tensor</em>, <em class="sig-param">norm_axis: int</em>, <em class="sig-param">epsilon: float</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="headerlink" href="#TorchSnippet.Layers.base.weight_norm_decompose" title="Permalink to this definition">¶</a></dt>
<dd><p>Decompose <cite>weight</cite> by “weight-norm”, i.e., into direction part <cite>v</cite>
and norm part <cite>v_norm</cite>.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>weight: The weight to be decomposed.
norm_axis: The axis, along with to calculate the weight norm.
epsilon: Infinitesimal constant to avoid dividing zero.</p>
</dd>
<dt>Returns:</dt><dd><p>A tuple of <cite>(v, v_norm)</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.base.NormedAndScaledWeightStore">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.base.</code><code class="sig-name descname">NormedAndScaledWeightStore</code><span class="sig-paren">(</span><em class="sig-param">shape: List[int], initializer: Union[int, float, numpy.ndarray, Callable[[...], None]], norm_axis: int = 1, epsilon: float = 1e-05</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.base.NormedAndScaledWeightStore" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#TorchSnippet.Layers.base.BaseParamStore" title="TorchSnippet.Layers.base.BaseParamStore"><code class="xref py py-class docutils literal notranslate"><span class="pre">TorchSnippet.Layers.base.BaseParamStore</span></code></a></p>
<p>A module that carries the weight-normed <cite>weight</cite>, with <cite>v</cite> and <cite>g</cite>.</p>
<dl class="attribute">
<dt id="TorchSnippet.Layers.base.NormedAndScaledWeightStore.epsilon">
<code class="sig-name descname">epsilon</code><em class="property">: float</em><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.NormedAndScaledWeightStore.epsilon" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="TorchSnippet.Layers.base.NormedAndScaledWeightStore.get">
<code class="sig-name descname">get</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#TorchSnippet.Layers.base.NormedAndScaledWeightStore.get" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.NormedAndScaledWeightStore.norm_axis">
<code class="sig-name descname">norm_axis</code><em class="property">: int</em><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.NormedAndScaledWeightStore.norm_axis" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="TorchSnippet.Layers.base.NormedAndScaledWeightStore.set">
<code class="sig-name descname">set</code><span class="sig-paren">(</span><em class="sig-param">value: Union[Tensor, numpy.ndarray, float, int]</em><span class="sig-paren">)</span> &#x2192; None<a class="headerlink" href="#TorchSnippet.Layers.base.NormedAndScaledWeightStore.set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="TorchSnippet.Layers.base.get_weight_store">
<code class="sig-prename descclassname">TorchSnippet.Layers.base.</code><code class="sig-name descname">get_weight_store</code><span class="sig-paren">(</span><em class="sig-param">shape: List[int], initializer: Union[int, float, numpy.ndarray, Callable[[...], None]] = &lt;function kaming_uniform&gt;, norm_axis: int = 1, weight_norm: Union[bool, str, TorchSnippet.typing_.WeightNormMode] = False</em><span class="sig-paren">)</span> &#x2192; TorchSnippet.Layers.base.BaseParamStore<a class="headerlink" href="#TorchSnippet.Layers.base.get_weight_store" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a module which carries the <cite>weight</cite> parameter.</p>
<dl>
<dt>Args:</dt><dd><p>shape: The shape of the weight.
initializer: The initializer for the weight.
norm_axis: The axis, along with to normalize the weight.
weight_norm: The mode of weight norm.</p>
<blockquote>
<div><p>Use <cite>NormedAndScaledWeightStore</cite> if <cite>True</cite> or <cite>WeightNormMode.FULL</cite>.
Use <cite>NormedWeightStore</cite> if <cite>WeightNormMode.NO_SCALE</cite>.
Use <cite>WeightStore</cite> if <cite>False</cite> or <cite>WeightNormMode.NONE</cite>.</p>
</div></blockquote>
</dd>
<dt>Returns:</dt><dd><p>The weight object.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="TorchSnippet.Layers.base.get_bias_store">
<code class="sig-prename descclassname">TorchSnippet.Layers.base.</code><code class="sig-name descname">get_bias_store</code><span class="sig-paren">(</span><em class="sig-param">shape: List[int], initializer: Union[int, float, numpy.ndarray, Callable[[...], None]] = &lt;function zeros&gt;, use_bias: bool = True</em><span class="sig-paren">)</span> &#x2192; Optional[TorchSnippet.Layers.base.BaseParamStore]<a class="headerlink" href="#TorchSnippet.Layers.base.get_bias_store" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a module that carries the <cite>bias</cite> parameter.</p>
<dl>
<dt>Args:</dt><dd><p>shape: The shape of the bias.
initializer: The initializer for the bias.
use_bias: Whether or not to use the bias?</p>
<blockquote>
<div><p>If <cite>False</cite>, will return <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>.</p>
</div></blockquote>
</dd>
<dt>Returns:</dt><dd><p>The bias object, or <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> if <cite>use_bias</cite> is False.</p>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.base.BaseLayer">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.base.</code><code class="sig-name descname">BaseLayer</code><a class="headerlink" href="#TorchSnippet.Layers.base.BaseLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>BaseLayer inherited from <cite>nn.Module</cite>.</p>
<dl class="method">
<dt id="TorchSnippet.Layers.base.BaseLayer.extra_repr">
<code class="sig-name descname">extra_repr</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; str<a class="headerlink" href="#TorchSnippet.Layers.base.BaseLayer.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should reimplement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.base.BaseSingleVariateLayer">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.base.</code><code class="sig-name descname">BaseSingleVariateLayer</code><a class="headerlink" href="#TorchSnippet.Layers.base.BaseSingleVariateLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#TorchSnippet.Layers.base.BaseLayer" title="TorchSnippet.Layers.base.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TorchSnippet.Layers.base.BaseLayer</span></code></a></p>
<p>Base class for single-input, single-output layers.</p>
<p>Sub-classes should override <cite>_call(input: Tensor) -&gt; Tensor</cite> to
actually implement the module.</p>
<dl class="method">
<dt id="TorchSnippet.Layers.base.BaseSingleVariateLayer.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#TorchSnippet.Layers.base.BaseSingleVariateLayer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.base.BaseMultiVariateLayer">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.base.</code><code class="sig-name descname">BaseMultiVariateLayer</code><a class="headerlink" href="#TorchSnippet.Layers.base.BaseMultiVariateLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#TorchSnippet.Layers.base.BaseLayer" title="TorchSnippet.Layers.base.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TorchSnippet.Layers.base.BaseLayer</span></code></a></p>
<p>Base class for multiple-input, multiple-output layers.
The inputs and outputs should be given as a list of Tensors.</p>
<dl class="method">
<dt id="TorchSnippet.Layers.base.BaseMultiVariateLayer.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">inputs: List[torch.Tensor]</em><span class="sig-paren">)</span> &#x2192; List[torch.Tensor]<a class="headerlink" href="#TorchSnippet.Layers.base.BaseMultiVariateLayer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.base.BaseSplitLayer">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.base.</code><code class="sig-name descname">BaseSplitLayer</code><a class="headerlink" href="#TorchSnippet.Layers.base.BaseSplitLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#TorchSnippet.Layers.base.BaseLayer" title="TorchSnippet.Layers.base.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TorchSnippet.Layers.base.BaseLayer</span></code></a></p>
<p>Base class for single-input, multiple-output layers.
The outputs should be given as a list of Tensors.</p>
<dl class="method">
<dt id="TorchSnippet.Layers.base.BaseSplitLayer.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; List[torch.Tensor]<a class="headerlink" href="#TorchSnippet.Layers.base.BaseSplitLayer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.base.BaseMergeLayer">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.base.</code><code class="sig-name descname">BaseMergeLayer</code><a class="headerlink" href="#TorchSnippet.Layers.base.BaseMergeLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#TorchSnippet.Layers.base.BaseLayer" title="TorchSnippet.Layers.base.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TorchSnippet.Layers.base.BaseLayer</span></code></a></p>
<p>Base class for multiple-input, single-output layers.
The inputs should be given as a list of Tensors.</p>
<dl class="method">
<dt id="TorchSnippet.Layers.base.BaseMergeLayer.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">inputs: List[torch.Tensor]</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#TorchSnippet.Layers.base.BaseMergeLayer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.base.BaseContextualLayer">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.base.</code><code class="sig-name descname">BaseContextualLayer</code><a class="headerlink" href="#TorchSnippet.Layers.base.BaseContextualLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#TorchSnippet.Layers.base.BaseLayer" title="TorchSnippet.Layers.base.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TorchSnippet.Layers.base.BaseLayer</span></code></a></p>
<p>Base class layers that produces the output according to the input tensor
and contextual tensors.</p>
<dl class="method">
<dt id="TorchSnippet.Layers.base.BaseContextualLayer.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">context: Optional[List[torch.Tensor]] = None</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#TorchSnippet.Layers.base.BaseContextualLayer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.base.BaseMultiVariateContextualLayer">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.base.</code><code class="sig-name descname">BaseMultiVariateContextualLayer</code><a class="headerlink" href="#TorchSnippet.Layers.base.BaseMultiVariateContextualLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#TorchSnippet.Layers.base.BaseLayer" title="TorchSnippet.Layers.base.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TorchSnippet.Layers.base.BaseLayer</span></code></a></p>
<p>Base class layers that produces the output tensors according to the
input tensors and contextual tensors.</p>
<dl class="method">
<dt id="TorchSnippet.Layers.base.BaseMultiVariateContextualLayer.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">inputs: List[torch.Tensor], context: Optional[List[torch.Tensor]] = None</em><span class="sig-paren">)</span> &#x2192; List[torch.Tensor]<a class="headerlink" href="#TorchSnippet.Layers.base.BaseMultiVariateContextualLayer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.base.Sequential">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.base.</code><code class="sig-name descname">Sequential</code><span class="sig-paren">(</span><em class="sig-param">*layers: Union[torch.nn.modules.module.Module, Sequence[torch.nn.modules.module.Module]]</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.base.Sequential" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.container.Sequential</span></code></p>
</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.base.CoreLinear">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.base.</code><code class="sig-name descname">CoreLinear</code><span class="sig-paren">(</span><em class="sig-param">weight_shape: List[int], bias_shape: List[int], use_bias: bool = True, weight_norm: Union[bool, str, TorchSnippet.typing_.WeightNormMode] = False, weight_init: Union[int, float, numpy.ndarray, Callable[[...], None]] = &lt;function kaming_uniform&gt;, bias_init: Union[int, float, numpy.ndarray, Callable[[...], None]] = &lt;function zeros&gt;, data_init: Union[Type[DataDependentInitializer], DataDependentInitializer, Callable[[...], DataDependentInitializer], None] = None</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.base.CoreLinear" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#TorchSnippet.Layers.base.BaseLayer" title="TorchSnippet.Layers.base.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TorchSnippet.Layers.base.BaseLayer</span></code></a></p>
<p>Base class for the core linear layers.</p>
<dl class="attribute">
<dt id="TorchSnippet.Layers.base.CoreLinear.bias_store">
<code class="sig-name descname">bias_store</code><em class="property">: Optional[Module]</em><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.CoreLinear.bias_store" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="TorchSnippet.Layers.base.CoreLinear.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#TorchSnippet.Layers.base.CoreLinear.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.CoreLinear.weight_store">
<code class="sig-name descname">weight_store</code><em class="property">: Module</em><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.CoreLinear.weight_store" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.base.Linear">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.base.</code><code class="sig-name descname">Linear</code><span class="sig-paren">(</span><em class="sig-param">in_features: int, out_features: int, use_bias: bool = True, weight_norm: Union[bool, str, TorchSnippet.typing_.WeightNormMode] = False, weight_init: Union[int, float, numpy.ndarray, Callable[[...], None]] = &lt;function kaming_uniform&gt;, bias_init: Union[int, float, numpy.ndarray, Callable[[...], None]] = &lt;function zeros&gt;, data_init: Union[Type[DataDependentInitializer], DataDependentInitializer, Callable[[...], DataDependentInitializer], None] = None</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.base.Linear" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#TorchSnippet.Layers.base.CoreLinear" title="TorchSnippet.Layers.base.CoreLinear"><code class="xref py py-class docutils literal notranslate"><span class="pre">TorchSnippet.Layers.base.CoreLinear</span></code></a></p>
<dl class="attribute">
<dt id="TorchSnippet.Layers.base.Linear.in_features">
<code class="sig-name descname">in_features</code><em class="property">: int</em><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.Linear.in_features" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.Linear.out_features">
<code class="sig-name descname">out_features</code><em class="property">: int</em><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.Linear.out_features" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.Linear.use_bias">
<code class="sig-name descname">use_bias</code><em class="property">: bool</em><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.Linear.use_bias" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.base.LinearConv1d">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.base.</code><code class="sig-name descname">LinearConv1d</code><span class="sig-paren">(</span><em class="sig-param">in_channels: int, out_channels: int, kernel_size: Union[int, Sequence[int]], stride: Union[int, Sequence[int]] = 1, padding: Union[Sequence[Union[Tuple[int, int], int]], int, str, TorchSnippet.typing_.PaddingMode] = &lt;PaddingMode.NONE: 'none'&gt;, dilation: Union[int, Sequence[int]] = 1, use_bias: bool = True, weight_norm: Union[bool, str, TorchSnippet.typing_.WeightNormMode] = False, weight_init: Union[int, float, numpy.ndarray, Callable[[...], None]] = &lt;function kaming_uniform&gt;, bias_init: Union[int, float, numpy.ndarray, Callable[[...], None]] = &lt;function zeros&gt;, data_init: Union[Type[DataDependentInitializer], DataDependentInitializer, Callable[[...], DataDependentInitializer], None] = None</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConv1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">TorchSnippet.Layers.base.LinearConvNd</span></code></p>
<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConv1d.dilation">
<code class="sig-name descname">dilation</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConv1d.dilation" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConv1d.in_channels">
<code class="sig-name descname">in_channels</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConv1d.in_channels" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConv1d.kernel_size">
<code class="sig-name descname">kernel_size</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConv1d.kernel_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConv1d.out_channels">
<code class="sig-name descname">out_channels</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConv1d.out_channels" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConv1d.padding">
<code class="sig-name descname">padding</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConv1d.padding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConv1d.stride">
<code class="sig-name descname">stride</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConv1d.stride" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConv1d.use_bias">
<code class="sig-name descname">use_bias</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConv1d.use_bias" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.base.LinearConv2d">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.base.</code><code class="sig-name descname">LinearConv2d</code><span class="sig-paren">(</span><em class="sig-param">in_channels: int, out_channels: int, kernel_size: Union[int, Sequence[int]], stride: Union[int, Sequence[int]] = 1, padding: Union[Sequence[Union[Tuple[int, int], int]], int, str, TorchSnippet.typing_.PaddingMode] = &lt;PaddingMode.NONE: 'none'&gt;, dilation: Union[int, Sequence[int]] = 1, use_bias: bool = True, weight_norm: Union[bool, str, TorchSnippet.typing_.WeightNormMode] = False, weight_init: Union[int, float, numpy.ndarray, Callable[[...], None]] = &lt;function kaming_uniform&gt;, bias_init: Union[int, float, numpy.ndarray, Callable[[...], None]] = &lt;function zeros&gt;, data_init: Union[Type[DataDependentInitializer], DataDependentInitializer, Callable[[...], DataDependentInitializer], None] = None</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConv2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">TorchSnippet.Layers.base.LinearConvNd</span></code></p>
<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConv2d.dilation">
<code class="sig-name descname">dilation</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConv2d.dilation" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConv2d.in_channels">
<code class="sig-name descname">in_channels</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConv2d.in_channels" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConv2d.kernel_size">
<code class="sig-name descname">kernel_size</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConv2d.kernel_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConv2d.out_channels">
<code class="sig-name descname">out_channels</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConv2d.out_channels" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConv2d.padding">
<code class="sig-name descname">padding</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConv2d.padding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConv2d.stride">
<code class="sig-name descname">stride</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConv2d.stride" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConv2d.use_bias">
<code class="sig-name descname">use_bias</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConv2d.use_bias" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.base.LinearConv3d">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.base.</code><code class="sig-name descname">LinearConv3d</code><span class="sig-paren">(</span><em class="sig-param">in_channels: int, out_channels: int, kernel_size: Union[int, Sequence[int]], stride: Union[int, Sequence[int]] = 1, padding: Union[Sequence[Union[Tuple[int, int], int]], int, str, TorchSnippet.typing_.PaddingMode] = &lt;PaddingMode.NONE: 'none'&gt;, dilation: Union[int, Sequence[int]] = 1, use_bias: bool = True, weight_norm: Union[bool, str, TorchSnippet.typing_.WeightNormMode] = False, weight_init: Union[int, float, numpy.ndarray, Callable[[...], None]] = &lt;function kaming_uniform&gt;, bias_init: Union[int, float, numpy.ndarray, Callable[[...], None]] = &lt;function zeros&gt;, data_init: Union[Type[DataDependentInitializer], DataDependentInitializer, Callable[[...], DataDependentInitializer], None] = None</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConv3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">TorchSnippet.Layers.base.LinearConvNd</span></code></p>
<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConv3d.dilation">
<code class="sig-name descname">dilation</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConv3d.dilation" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConv3d.in_channels">
<code class="sig-name descname">in_channels</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConv3d.in_channels" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConv3d.kernel_size">
<code class="sig-name descname">kernel_size</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConv3d.kernel_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConv3d.out_channels">
<code class="sig-name descname">out_channels</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConv3d.out_channels" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConv3d.padding">
<code class="sig-name descname">padding</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConv3d.padding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConv3d.stride">
<code class="sig-name descname">stride</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConv3d.stride" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConv3d.use_bias">
<code class="sig-name descname">use_bias</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConv3d.use_bias" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.base.LinearConvTranspose1d">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.base.</code><code class="sig-name descname">LinearConvTranspose1d</code><span class="sig-paren">(</span><em class="sig-param">in_channels: int, out_channels: int, kernel_size: Union[int, Sequence[int]], stride: Union[int, Sequence[int]] = 1, padding: Union[Sequence[Union[Tuple[int, int], int]], int, str, TorchSnippet.typing_.PaddingMode] = &lt;PaddingMode.NONE: 'none'&gt;, output_padding: Union[int, Sequence[int]] = 0, dilation: Union[int, Sequence[int]] = 1, use_bias: bool = True, weight_norm: Union[bool, str, TorchSnippet.typing_.WeightNormMode] = False, weight_init: Union[int, float, numpy.ndarray, Callable[[...], None]] = &lt;function kaming_uniform&gt;, bias_init: Union[int, float, numpy.ndarray, Callable[[...], None]] = &lt;function zeros&gt;, data_init: Union[Type[DataDependentInitializer], DataDependentInitializer, Callable[[...], DataDependentInitializer], None] = None</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConvTranspose1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">TorchSnippet.Layers.base.LinearConvTransposeNd</span></code></p>
<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConvTranspose1d.dilation">
<code class="sig-name descname">dilation</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConvTranspose1d.dilation" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConvTranspose1d.in_channels">
<code class="sig-name descname">in_channels</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConvTranspose1d.in_channels" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConvTranspose1d.is_symmetric_padding">
<code class="sig-name descname">is_symmetric_padding</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConvTranspose1d.is_symmetric_padding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConvTranspose1d.kernel_size">
<code class="sig-name descname">kernel_size</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConvTranspose1d.kernel_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConvTranspose1d.out_channels">
<code class="sig-name descname">out_channels</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConvTranspose1d.out_channels" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConvTranspose1d.output_padding">
<code class="sig-name descname">output_padding</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConvTranspose1d.output_padding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConvTranspose1d.padding">
<code class="sig-name descname">padding</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConvTranspose1d.padding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConvTranspose1d.stride">
<code class="sig-name descname">stride</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConvTranspose1d.stride" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConvTranspose1d.use_bias">
<code class="sig-name descname">use_bias</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConvTranspose1d.use_bias" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.base.LinearConvTranspose2d">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.base.</code><code class="sig-name descname">LinearConvTranspose2d</code><span class="sig-paren">(</span><em class="sig-param">in_channels: int, out_channels: int, kernel_size: Union[int, Sequence[int]], stride: Union[int, Sequence[int]] = 1, padding: Union[Sequence[Union[Tuple[int, int], int]], int, str, TorchSnippet.typing_.PaddingMode] = &lt;PaddingMode.NONE: 'none'&gt;, output_padding: Union[int, Sequence[int]] = 0, dilation: Union[int, Sequence[int]] = 1, use_bias: bool = True, weight_norm: Union[bool, str, TorchSnippet.typing_.WeightNormMode] = False, weight_init: Union[int, float, numpy.ndarray, Callable[[...], None]] = &lt;function kaming_uniform&gt;, bias_init: Union[int, float, numpy.ndarray, Callable[[...], None]] = &lt;function zeros&gt;, data_init: Union[Type[DataDependentInitializer], DataDependentInitializer, Callable[[...], DataDependentInitializer], None] = None</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConvTranspose2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">TorchSnippet.Layers.base.LinearConvTransposeNd</span></code></p>
<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConvTranspose2d.dilation">
<code class="sig-name descname">dilation</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConvTranspose2d.dilation" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConvTranspose2d.in_channels">
<code class="sig-name descname">in_channels</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConvTranspose2d.in_channels" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConvTranspose2d.is_symmetric_padding">
<code class="sig-name descname">is_symmetric_padding</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConvTranspose2d.is_symmetric_padding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConvTranspose2d.kernel_size">
<code class="sig-name descname">kernel_size</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConvTranspose2d.kernel_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConvTranspose2d.out_channels">
<code class="sig-name descname">out_channels</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConvTranspose2d.out_channels" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConvTranspose2d.output_padding">
<code class="sig-name descname">output_padding</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConvTranspose2d.output_padding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConvTranspose2d.padding">
<code class="sig-name descname">padding</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConvTranspose2d.padding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConvTranspose2d.stride">
<code class="sig-name descname">stride</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConvTranspose2d.stride" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConvTranspose2d.use_bias">
<code class="sig-name descname">use_bias</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConvTranspose2d.use_bias" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.base.LinearConvTranspose3d">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.base.</code><code class="sig-name descname">LinearConvTranspose3d</code><span class="sig-paren">(</span><em class="sig-param">in_channels: int, out_channels: int, kernel_size: Union[int, Sequence[int]], stride: Union[int, Sequence[int]] = 1, padding: Union[Sequence[Union[Tuple[int, int], int]], int, str, TorchSnippet.typing_.PaddingMode] = &lt;PaddingMode.NONE: 'none'&gt;, output_padding: Union[int, Sequence[int]] = 0, dilation: Union[int, Sequence[int]] = 1, use_bias: bool = True, weight_norm: Union[bool, str, TorchSnippet.typing_.WeightNormMode] = False, weight_init: Union[int, float, numpy.ndarray, Callable[[...], None]] = &lt;function kaming_uniform&gt;, bias_init: Union[int, float, numpy.ndarray, Callable[[...], None]] = &lt;function zeros&gt;, data_init: Union[Type[DataDependentInitializer], DataDependentInitializer, Callable[[...], DataDependentInitializer], None] = None</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConvTranspose3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">TorchSnippet.Layers.base.LinearConvTransposeNd</span></code></p>
<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConvTranspose3d.dilation">
<code class="sig-name descname">dilation</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConvTranspose3d.dilation" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConvTranspose3d.in_channels">
<code class="sig-name descname">in_channels</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConvTranspose3d.in_channels" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConvTranspose3d.is_symmetric_padding">
<code class="sig-name descname">is_symmetric_padding</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConvTranspose3d.is_symmetric_padding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConvTranspose3d.kernel_size">
<code class="sig-name descname">kernel_size</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConvTranspose3d.kernel_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConvTranspose3d.out_channels">
<code class="sig-name descname">out_channels</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConvTranspose3d.out_channels" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConvTranspose3d.output_padding">
<code class="sig-name descname">output_padding</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConvTranspose3d.output_padding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConvTranspose3d.padding">
<code class="sig-name descname">padding</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConvTranspose3d.padding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConvTranspose3d.stride">
<code class="sig-name descname">stride</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConvTranspose3d.stride" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.base.LinearConvTranspose3d.use_bias">
<code class="sig-name descname">use_bias</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.LinearConvTranspose3d.use_bias" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.base.BatchNorm">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.base.</code><code class="sig-name descname">BatchNorm</code><span class="sig-paren">(</span><em class="sig-param">num_features: int</em>, <em class="sig-param">momentum: float = 0.1</em>, <em class="sig-param">epsilon: float = 1e-05</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.base.BatchNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.batchnorm.BatchNorm1d</span></code></p>
<p>Batch normalization for dense layers.</p>
</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.base.BatchNorm1d">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.base.</code><code class="sig-name descname">BatchNorm1d</code><span class="sig-paren">(</span><em class="sig-param">num_features: int</em>, <em class="sig-param">momentum: float = 0.1</em>, <em class="sig-param">epsilon: float = 1e-05</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.base.BatchNorm1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.batchnorm.BatchNorm1d</span></code></p>
<p>Batch normalization for 1D convolutional layers.</p>
</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.base.BatchNorm2d">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.base.</code><code class="sig-name descname">BatchNorm2d</code><span class="sig-paren">(</span><em class="sig-param">num_features: int</em>, <em class="sig-param">momentum: float = 0.1</em>, <em class="sig-param">epsilon: float = 1e-05</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.base.BatchNorm2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.batchnorm.BatchNorm2d</span></code></p>
<p>Batch normalization for 2D convolutional layers.</p>
</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.base.BatchNorm3d">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.base.</code><code class="sig-name descname">BatchNorm3d</code><span class="sig-paren">(</span><em class="sig-param">num_features: int</em>, <em class="sig-param">momentum: float = 0.1</em>, <em class="sig-param">epsilon: float = 1e-05</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.base.BatchNorm3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.batchnorm.BatchNorm3d</span></code></p>
<p>Batch normalization for 3D convolutional layers.</p>
</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.base.Dropout">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.base.</code><code class="sig-name descname">Dropout</code><span class="sig-paren">(</span><em class="sig-param">p=0.5</em>, <em class="sig-param">inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.base.Dropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.dropout._DropoutNd</span></code></p>
<p>During training, randomly zeroes some of the elements of the input
tensor with probability <code class="xref py py-attr docutils literal notranslate"><span class="pre">p</span></code> using samples from a Bernoulli
distribution. Each channel will be zeroed out independently on every forward
call.</p>
<p>This has proven to be an effective technique for regularization and
preventing the co-adaptation of neurons as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1207.0580">Improving neural networks by preventing co-adaptation of feature
detectors</a> .</p>
<p>Furthermore, the outputs are scaled by a factor of <span class="math notranslate nohighlight">\(\frac{1}{1-p}\)</span> during
training. This means that during evaluation the module simply computes an
identity function.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>p: probability of an element to be zeroed. Default: 0.5
inplace: If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, will do this operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
</dd>
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((*)\)</span>. Input can be of any shape</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((*)\)</span>. Output is of the same shape as input</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<dl class="method">
<dt id="TorchSnippet.Layers.base.Dropout.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.base.Dropout.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.base.Dropout1d">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.base.</code><code class="sig-name descname">Dropout1d</code><span class="sig-paren">(</span><em class="sig-param">p: float = 0.5</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.base.Dropout1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#TorchSnippet.Layers.base.BaseSingleVariateLayer" title="TorchSnippet.Layers.base.BaseSingleVariateLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TorchSnippet.Layers.base.BaseSingleVariateLayer</span></code></a></p>
<p>Randomly zero out entire channels of the 1d convolution input.</p>
<dl class="attribute">
<dt id="TorchSnippet.Layers.base.Dropout1d.p">
<code class="sig-name descname">p</code><em class="property">: float</em><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.base.Dropout1d.p" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.base.Dropout2d">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.base.</code><code class="sig-name descname">Dropout2d</code><span class="sig-paren">(</span><em class="sig-param">p=0.5</em>, <em class="sig-param">inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.base.Dropout2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.dropout._DropoutNd</span></code></p>
<p>Randomly zero out entire channels (a channel is a 2D feature map,
e.g., the <span class="math notranslate nohighlight">\(j\)</span>-th channel of the <span class="math notranslate nohighlight">\(i\)</span>-th sample in the
batched input is a 2D tensor <span class="math notranslate nohighlight">\(\text{input}[i, j]\)</span>).
Each channel will be zeroed out independently on every forward call with
probability <code class="xref py py-attr docutils literal notranslate"><span class="pre">p</span></code> using samples from a Bernoulli distribution.</p>
<p>Usually the input comes from <code class="xref py py-class docutils literal notranslate"><span class="pre">nn.Conv2d</span></code> modules.</p>
<p>As described in the paper
<a class="reference external" href="http://arxiv.org/abs/1411.4280">Efficient Object Localization Using Convolutional Networks</a> ,
if adjacent pixels within feature maps are strongly correlated
(as is normally the case in early convolution layers) then i.i.d. dropout
will not regularize the activations and will otherwise just result
in an effective learning rate decrease.</p>
<p>In this case, <code class="xref py py-func docutils literal notranslate"><span class="pre">nn.Dropout2d()</span></code> will help promote independence between
feature maps and should be used instead.</p>
<dl>
<dt>Args:</dt><dd><p>p (float, optional): probability of an element to be zero-ed.
inplace (bool, optional): If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, will do this operation</p>
<blockquote>
<div><p>in-place</p>
</div></blockquote>
</dd>
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, H, W)\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, H, W)\)</span> (same shape as input)</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout2d</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<dl class="method">
<dt id="TorchSnippet.Layers.base.Dropout2d.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.base.Dropout2d.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.base.Dropout3d">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.base.</code><code class="sig-name descname">Dropout3d</code><span class="sig-paren">(</span><em class="sig-param">p=0.5</em>, <em class="sig-param">inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.base.Dropout3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.dropout._DropoutNd</span></code></p>
<p>Randomly zero out entire channels (a channel is a 3D feature map,
e.g., the <span class="math notranslate nohighlight">\(j\)</span>-th channel of the <span class="math notranslate nohighlight">\(i\)</span>-th sample in the
batched input is a 3D tensor <span class="math notranslate nohighlight">\(\text{input}[i, j]\)</span>).
Each channel will be zeroed out independently on every forward call with
probability <code class="xref py py-attr docutils literal notranslate"><span class="pre">p</span></code> using samples from a Bernoulli distribution.</p>
<p>Usually the input comes from <code class="xref py py-class docutils literal notranslate"><span class="pre">nn.Conv3d</span></code> modules.</p>
<p>As described in the paper
<a class="reference external" href="http://arxiv.org/abs/1411.4280">Efficient Object Localization Using Convolutional Networks</a> ,
if adjacent pixels within feature maps are strongly correlated
(as is normally the case in early convolution layers) then i.i.d. dropout
will not regularize the activations and will otherwise just result
in an effective learning rate decrease.</p>
<p>In this case, <code class="xref py py-func docutils literal notranslate"><span class="pre">nn.Dropout3d()</span></code> will help promote independence between
feature maps and should be used instead.</p>
<dl>
<dt>Args:</dt><dd><p>p (float, optional): probability of an element to be zeroed.
inplace (bool, optional): If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, will do this operation</p>
<blockquote>
<div><p>in-place</p>
</div></blockquote>
</dd>
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span> (same shape as input)</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout3d</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<dl class="method">
<dt id="TorchSnippet.Layers.base.Dropout3d.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.base.Dropout3d.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-TorchSnippet.Layers.gated">
<span id="torchsnippet-layers-gated-module"></span><h2>TorchSnippet.Layers.gated module<a class="headerlink" href="#module-TorchSnippet.Layers.gated" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="TorchSnippet.Layers.gated.Gated">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.gated.</code><code class="sig-name descname">Gated</code><span class="sig-paren">(</span><em class="sig-param">feature_axis: int</em>, <em class="sig-param">num_features: int</em>, <em class="sig-param">gate_bias: float</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.gated.Gated" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">TorchSnippet.Layers.gated.BaseGated</span></code></p>
<dl class="attribute">
<dt id="TorchSnippet.Layers.gated.Gated.feature_axis">
<code class="sig-name descname">feature_axis</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.gated.Gated.feature_axis" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.gated.Gated.gate_bias">
<code class="sig-name descname">gate_bias</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.gated.Gated.gate_bias" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.Layers.gated.Gated.num_features">
<code class="sig-name descname">num_features</code><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.gated.Gated.num_features" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.gated.GatedWithActivation">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.gated.</code><code class="sig-name descname">GatedWithActivation</code><span class="sig-paren">(</span><em class="sig-param">feature_axis: int</em>, <em class="sig-param">num_features: int</em>, <em class="sig-param">gate_bias: float</em>, <em class="sig-param">activation: torch.nn.modules.module.Module</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.gated.GatedWithActivation" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">TorchSnippet.Layers.gated.BaseGated</span></code></p>
<dl class="attribute">
<dt id="TorchSnippet.Layers.gated.GatedWithActivation.activation">
<code class="sig-name descname">activation</code><em class="property">: Module</em><em class="property"> = None</em><a class="headerlink" href="#TorchSnippet.Layers.gated.GatedWithActivation.activation" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-TorchSnippet.Layers.initializer">
<span id="torchsnippet-layers-initializer-module"></span><h2>TorchSnippet.Layers.initializer module<a class="headerlink" href="#module-TorchSnippet.Layers.initializer" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="TorchSnippet.Layers.initializer.calculate_fan_in_and_fan_out">
<code class="sig-prename descclassname">TorchSnippet.Layers.initializer.</code><code class="sig-name descname">calculate_fan_in_and_fan_out</code><span class="sig-paren">(</span><em class="sig-param">tensor: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[int, int]<a class="headerlink" href="#TorchSnippet.Layers.initializer.calculate_fan_in_and_fan_out" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.Layers.initializer.get_activation_gain">
<code class="sig-prename descclassname">TorchSnippet.Layers.initializer.</code><code class="sig-name descname">get_activation_gain</code><span class="sig-paren">(</span><em class="sig-param">activation: Union[Type[torch.nn.modules.module.Module], torch.nn.modules.module.Module, None], *args, **kwargs</em><span class="sig-paren">)</span> &#x2192; float<a class="headerlink" href="#TorchSnippet.Layers.initializer.get_activation_gain" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the preferred initialization gain for specified activation.</p>
<dl>
<dt>Args:</dt><dd><p>activation: The class or instance of the activation module.
<a href="#id2"><span class="problematic" id="id3">*</span></a>args, **kwargs: Arguments to construct the activation module,</p>
<blockquote>
<div><p>if the specified <cite>activation</cite> is given as its class.
Ignored if <cite>activation</cite> is already a module.</p>
</div></blockquote>
</dd>
<dt>Returns:</dt><dd><p>The initialization gain.  If <cite>activation</cite> is not recognized, returns 1.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="TorchSnippet.Layers.initializer.apply_initializer">
<code class="sig-prename descclassname">TorchSnippet.Layers.initializer.</code><code class="sig-name descname">apply_initializer</code><span class="sig-paren">(</span><em class="sig-param">tensor: torch.Tensor, initializer: Union[int, float, numpy.ndarray, torch.Tensor, Callable[[...], None], None], gain: Optional[float] = None, activation: Union[str, Type[torch.nn.modules.module.Module], torch.nn.modules.module.Module, Any, None] = None, fan_in_and_fan_out: Optional[Tuple[int, int]] = None, mode: str = 'fan_in'</em><span class="sig-paren">)</span> &#x2192; None<a class="headerlink" href="#TorchSnippet.Layers.initializer.apply_initializer" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply an <cite>initializer</cite> on the specified <cite>tensor</cite>.</p>
<dl>
<dt>Args:</dt><dd><p>tensor: The tensor to be initialized.
initializer: The initializer, may be one of:</p>
<blockquote>
<div><ul class="simple">
<li><p>A scalar, which will be filled into <cite>tensor</cite>.</p></li>
<li><p>A NumPy array or another <cite>Tensor</cite>, whose value will be copied
to the <cite>tensor</cite>.</p></li>
<li><p>A callable function <code class="docutils literal notranslate"><span class="pre">(t:</span> <span class="pre">Tensor,</span> <span class="pre">\**kwargs)</span> <span class="pre">-&gt;</span> <span class="pre">None</span></code>.
The <cite>**kwargs</cite> must present in order to consume all
named arguments passed to the initializer.  Currently
possible named arguments are: <cite>gain</cite>, <cite>fan_in_and_fan_out</cite>,
and <cite>mode</cite>.</p></li>
</ul>
</div></blockquote>
<dl class="simple">
<dt>gain: The gain of the activation.  If not specified, will calculate</dt><dd><p>according to <cite>activation</cite> via <a class="reference internal" href="#TorchSnippet.Layers.initializer.get_activation_gain" title="TorchSnippet.Layers.initializer.get_activation_gain"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_activation_gain()</span></code></a>.</p>
</dd>
</dl>
<p>activation: The activation of the layer.
fan_in_and_fan_out: A tuple of <code class="docutils literal notranslate"><span class="pre">(fan_in,</span> <span class="pre">fan_out)</span></code> of the layer.</p>
<blockquote>
<div><p>If not specified, and if <cite>rank(tensor)</cite> &gt;= 2, it will be computed
via <a class="reference internal" href="#TorchSnippet.Layers.initializer.calculate_fan_in_and_fan_out" title="TorchSnippet.Layers.initializer.calculate_fan_in_and_fan_out"><code class="xref py py-func docutils literal notranslate"><span class="pre">calculate_fan_in_and_fan_out()</span></code></a>.</p>
</div></blockquote>
<dl class="simple">
<dt>mode: Either “fan_in” or “fan_out”.  If it is “fan_out”, then the</dt><dd><p>specified or calculated <cite>fan_in</cite> will be regarded as <cite>fan_out</cite>,
and <cite>fan_out</cite> regarded as <cite>fan_in</cite>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="TorchSnippet.Layers.initializer.zeros">
<code class="sig-prename descclassname">TorchSnippet.Layers.initializer.</code><code class="sig-name descname">zeros</code><span class="sig-paren">(</span><em class="sig-param">tensor: torch.Tensor</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.initializer.zeros" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.Layers.initializer.ones">
<code class="sig-prename descclassname">TorchSnippet.Layers.initializer.</code><code class="sig-name descname">ones</code><span class="sig-paren">(</span><em class="sig-param">tensor: torch.Tensor</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.initializer.ones" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.Layers.initializer.fill">
<code class="sig-prename descclassname">TorchSnippet.Layers.initializer.</code><code class="sig-name descname">fill</code><span class="sig-paren">(</span><em class="sig-param">tensor: torch.Tensor, fill_value: Union[int, float, numpy.ndarray], **kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.initializer.fill" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.Layers.initializer.uniform">
<code class="sig-prename descclassname">TorchSnippet.Layers.initializer.</code><code class="sig-name descname">uniform</code><span class="sig-paren">(</span><em class="sig-param">tensor: torch.Tensor</em>, <em class="sig-param">low: float = 0.0</em>, <em class="sig-param">high: float = 1.0</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.initializer.uniform" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.Layers.initializer.normal">
<code class="sig-prename descclassname">TorchSnippet.Layers.initializer.</code><code class="sig-name descname">normal</code><span class="sig-paren">(</span><em class="sig-param">tensor: torch.Tensor</em>, <em class="sig-param">mean: float = 0.0</em>, <em class="sig-param">std: float = 1.0</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.initializer.normal" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.Layers.initializer.xavier_uniform">
<code class="sig-prename descclassname">TorchSnippet.Layers.initializer.</code><code class="sig-name descname">xavier_uniform</code><span class="sig-paren">(</span><em class="sig-param">tensor: torch.Tensor</em>, <em class="sig-param">gain: float = 1.0</em>, <em class="sig-param">fan_in_and_fan_out: Optional[Tuple[int</em>, <em class="sig-param">int]] = None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.initializer.xavier_uniform" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.Layers.initializer.xavier_normal">
<code class="sig-prename descclassname">TorchSnippet.Layers.initializer.</code><code class="sig-name descname">xavier_normal</code><span class="sig-paren">(</span><em class="sig-param">tensor: torch.Tensor</em>, <em class="sig-param">gain: float = 1.0</em>, <em class="sig-param">fan_in_and_fan_out: Optional[Tuple[int</em>, <em class="sig-param">int]] = None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.initializer.xavier_normal" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.Layers.initializer.kaming_uniform">
<code class="sig-prename descclassname">TorchSnippet.Layers.initializer.</code><code class="sig-name descname">kaming_uniform</code><span class="sig-paren">(</span><em class="sig-param">tensor: torch.Tensor</em>, <em class="sig-param">gain: float = 1.0</em>, <em class="sig-param">fan_in_and_fan_out: Optional[Tuple[int</em>, <em class="sig-param">int]] = None</em>, <em class="sig-param">mode: str = 'fan_in'</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.initializer.kaming_uniform" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.Layers.initializer.kaming_normal">
<code class="sig-prename descclassname">TorchSnippet.Layers.initializer.</code><code class="sig-name descname">kaming_normal</code><span class="sig-paren">(</span><em class="sig-param">tensor: torch.Tensor</em>, <em class="sig-param">gain: float = 1.0</em>, <em class="sig-param">fan_in_and_fan_out: Optional[Tuple[int</em>, <em class="sig-param">int]] = None</em>, <em class="sig-param">mode: str = 'fan_in'</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.initializer.kaming_normal" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.initializer.DataDependentInitializer">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.initializer.</code><code class="sig-name descname">DataDependentInitializer</code><a class="headerlink" href="#TorchSnippet.Layers.initializer.DataDependentInitializer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Base class for data-dependent initializers.</p>
<p>A <a class="reference internal" href="#TorchSnippet.Layers.initializer.DataDependentInitializer" title="TorchSnippet.Layers.initializer.DataDependentInitializer"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataDependentInitializer</span></code></a> initializes the <cite>weight</cite> and <cite>bias</cite> of
layers according to their inputs.  <a class="reference internal" href="#TorchSnippet.Layers.initializer.DataDependentInitializer" title="TorchSnippet.Layers.initializer.DataDependentInitializer"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataDependentInitializer</span></code></a> are
generally stateless, and can be shared among layers.</p>
<dl class="method">
<dt id="TorchSnippet.Layers.initializer.DataDependentInitializer.register">
<code class="sig-name descname">register</code><span class="sig-paren">(</span><em class="sig-param">layer: torch.nn.modules.module.Module</em>, <em class="sig-param">initialized: bool = False</em><span class="sig-paren">)</span> &#x2192; None<a class="headerlink" href="#TorchSnippet.Layers.initializer.DataDependentInitializer.register" title="Permalink to this definition">¶</a></dt>
<dd><p>Register this data-dependent initializer to the specified <cite>layer</cite>.</p>
<dl>
<dt>Args:</dt><dd><p>layer: The layer to be initialized by this initializer.
initialized: The initial <cite>initialized</cite> flag of the hook.</p>
<blockquote>
<div><p>Defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>.</p>
</div></blockquote>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="TorchSnippet.Layers.initializer.set_initialized">
<code class="sig-prename descclassname">TorchSnippet.Layers.initializer.</code><code class="sig-name descname">set_initialized</code><span class="sig-paren">(</span><em class="sig-param">root: torch.nn.modules.module.Module</em>, <em class="sig-param">initialized: bool = True</em><span class="sig-paren">)</span> &#x2192; None<a class="headerlink" href="#TorchSnippet.Layers.initializer.set_initialized" title="Permalink to this definition">¶</a></dt>
<dd><p>Call <cite>set_initialized</cite> on <cite>root</cite> and all its children layers (recursively),
as well as their every data-dependent initializer hook.</p>
<dl>
<dt>Args:</dt><dd><p>root: The root layer.
initialized: The value of the <cite>initialized</cite> flag.</p>
<blockquote>
<div><p>If <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>, the data-dependent initializers will be disabled.
If <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>, the data-dependent initializes will be enabled
for the next forward call.</p>
</div></blockquote>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="TorchSnippet.Layers.initializer.remove_data_dependent_initializers">
<code class="sig-prename descclassname">TorchSnippet.Layers.initializer.</code><code class="sig-name descname">remove_data_dependent_initializers</code><span class="sig-paren">(</span><em class="sig-param">root: torch.nn.modules.module.Module</em><span class="sig-paren">)</span> &#x2192; None<a class="headerlink" href="#TorchSnippet.Layers.initializer.remove_data_dependent_initializers" title="Permalink to this definition">¶</a></dt>
<dd><p>Remove all data-dependent initializer hooks from the <cite>root</cite> module and all
its children (recursively).</p>
<dl class="simple">
<dt>Args:</dt><dd><p>root: The root module.</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-TorchSnippet.Layers.layers">
<span id="torchsnippet-layers-layers-module"></span><h2>TorchSnippet.Layers.layers module<a class="headerlink" href="#module-TorchSnippet.Layers.layers" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="TorchSnippet.Layers.layers.Dense">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.layers.</code><code class="sig-name descname">Dense</code><span class="sig-paren">(</span><em class="sig-param">in_features: int, out_features: int, use_bias: Optional[bool] = None, normalizer: Union[Module, Type[Module], Callable[[int], Module], None] = None, activation: Union[Module, Type[Module], Callable[[], Module], None] = None, weight_norm: Union[bool, str, TorchSnippet.typing_.WeightNormMode] = False, gated: bool = False, gate_bias: float = 2.0, weight_init: Union[int, float, numpy.ndarray, Callable[[...], None]] = &lt;function kaming_uniform&gt;, bias_init: Union[int, float, numpy.ndarray, Callable[[...], None]] = &lt;function zeros&gt;, data_init: Union[Type[DataDependentInitializer], DataDependentInitializer, Callable[[...], DataDependentInitializer], None] = None</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.layers.Dense" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#TorchSnippet.Layers.base.Sequential" title="TorchSnippet.Layers.base.Sequential"><code class="xref py py-class docutils literal notranslate"><span class="pre">TorchSnippet.Layers.base.Sequential</span></code></a></p>
</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.layers.Conv1d">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.layers.</code><code class="sig-name descname">Conv1d</code><span class="sig-paren">(</span><em class="sig-param">in_channels: int, out_channels: int, kernel_size: Union[int, Sequence[int]], stride: Union[int, Sequence[int]] = 1, padding: Union[Sequence[Union[Tuple[int, int], int]], int, str, TorchSnippet.typing_.PaddingMode] = &lt;PaddingMode.NONE: 'none'&gt;, dilation: Union[int, Sequence[int]] = 1, use_bias: Optional[bool] = None, activation: Union[Module, Type[Module], Callable[[], Module], None] = None, normalizer: Union[Module, Type[Module], Callable[[int], Module], None] = None, weight_norm: Union[bool, str, TorchSnippet.typing_.WeightNormMode] = False, gated: bool = False, gate_bias: float = 2.0, weight_init: Union[int, float, numpy.ndarray, Callable[[...], None]] = &lt;function kaming_uniform&gt;, bias_init: Union[int, float, numpy.ndarray, Callable[[...], None]] = &lt;function zeros&gt;, data_init: Union[Type[DataDependentInitializer], DataDependentInitializer, Callable[[...], DataDependentInitializer], None] = None</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.layers.Conv1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">TorchSnippet.Layers.layers.ConvNd</span></code></p>
</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.layers.Conv2d">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.layers.</code><code class="sig-name descname">Conv2d</code><span class="sig-paren">(</span><em class="sig-param">in_channels: int, out_channels: int, kernel_size: Union[int, Sequence[int]], stride: Union[int, Sequence[int]] = 1, padding: Union[Sequence[Union[Tuple[int, int], int]], int, str, TorchSnippet.typing_.PaddingMode] = &lt;PaddingMode.NONE: 'none'&gt;, dilation: Union[int, Sequence[int]] = 1, use_bias: Optional[bool] = None, activation: Union[Module, Type[Module], Callable[[], Module], None] = None, normalizer: Union[Module, Type[Module], Callable[[int], Module], None] = None, weight_norm: Union[bool, str, TorchSnippet.typing_.WeightNormMode] = False, gated: bool = False, gate_bias: float = 2.0, weight_init: Union[int, float, numpy.ndarray, Callable[[...], None]] = &lt;function kaming_uniform&gt;, bias_init: Union[int, float, numpy.ndarray, Callable[[...], None]] = &lt;function zeros&gt;, data_init: Union[Type[DataDependentInitializer], DataDependentInitializer, Callable[[...], DataDependentInitializer], None] = None</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.layers.Conv2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">TorchSnippet.Layers.layers.ConvNd</span></code></p>
</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.layers.Conv3d">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.layers.</code><code class="sig-name descname">Conv3d</code><span class="sig-paren">(</span><em class="sig-param">in_channels: int, out_channels: int, kernel_size: Union[int, Sequence[int]], stride: Union[int, Sequence[int]] = 1, padding: Union[Sequence[Union[Tuple[int, int], int]], int, str, TorchSnippet.typing_.PaddingMode] = &lt;PaddingMode.NONE: 'none'&gt;, dilation: Union[int, Sequence[int]] = 1, use_bias: Optional[bool] = None, activation: Union[Module, Type[Module], Callable[[], Module], None] = None, normalizer: Union[Module, Type[Module], Callable[[int], Module], None] = None, weight_norm: Union[bool, str, TorchSnippet.typing_.WeightNormMode] = False, gated: bool = False, gate_bias: float = 2.0, weight_init: Union[int, float, numpy.ndarray, Callable[[...], None]] = &lt;function kaming_uniform&gt;, bias_init: Union[int, float, numpy.ndarray, Callable[[...], None]] = &lt;function zeros&gt;, data_init: Union[Type[DataDependentInitializer], DataDependentInitializer, Callable[[...], DataDependentInitializer], None] = None</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.layers.Conv3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">TorchSnippet.Layers.layers.ConvNd</span></code></p>
</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.layers.ConvTranspose1d">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.layers.</code><code class="sig-name descname">ConvTranspose1d</code><span class="sig-paren">(</span><em class="sig-param">in_channels: int, out_channels: int, kernel_size: Union[int, Sequence[int]], stride: Union[int, Sequence[int]] = 1, padding: Union[Sequence[Union[Tuple[int, int], int]], int, str, TorchSnippet.typing_.PaddingMode] = &lt;PaddingMode.NONE: 'none'&gt;, output_padding: Union[int, Sequence[int]] = 0, dilation: Union[int, Sequence[int]] = 1, use_bias: Optional[bool] = None, activation: Union[Module, Type[Module], Callable[[], Module], None] = None, normalizer: Union[Module, Type[Module], Callable[[int], Module], None] = None, weight_norm: Union[bool, str, TorchSnippet.typing_.WeightNormMode] = False, gated: bool = False, gate_bias: float = 2.0, weight_init: Union[int, float, numpy.ndarray, Callable[[...], None]] = &lt;function kaming_uniform&gt;, bias_init: Union[int, float, numpy.ndarray, Callable[[...], None]] = &lt;function zeros&gt;, data_init: Union[Type[DataDependentInitializer], DataDependentInitializer, Callable[[...], DataDependentInitializer], None] = None</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.layers.ConvTranspose1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">TorchSnippet.Layers.layers.ConvTransposeNd</span></code></p>
</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.layers.ConvTranspose2d">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.layers.</code><code class="sig-name descname">ConvTranspose2d</code><span class="sig-paren">(</span><em class="sig-param">in_channels: int, out_channels: int, kernel_size: Union[int, Sequence[int]], stride: Union[int, Sequence[int]] = 1, padding: Union[Sequence[Union[Tuple[int, int], int]], int, str, TorchSnippet.typing_.PaddingMode] = &lt;PaddingMode.NONE: 'none'&gt;, output_padding: Union[int, Sequence[int]] = 0, dilation: Union[int, Sequence[int]] = 1, use_bias: Optional[bool] = None, activation: Union[Module, Type[Module], Callable[[], Module], None] = None, normalizer: Union[Module, Type[Module], Callable[[int], Module], None] = None, weight_norm: Union[bool, str, TorchSnippet.typing_.WeightNormMode] = False, gated: bool = False, gate_bias: float = 2.0, weight_init: Union[int, float, numpy.ndarray, Callable[[...], None]] = &lt;function kaming_uniform&gt;, bias_init: Union[int, float, numpy.ndarray, Callable[[...], None]] = &lt;function zeros&gt;, data_init: Union[Type[DataDependentInitializer], DataDependentInitializer, Callable[[...], DataDependentInitializer], None] = None</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.layers.ConvTranspose2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">TorchSnippet.Layers.layers.ConvTransposeNd</span></code></p>
</dd></dl>

<dl class="class">
<dt id="TorchSnippet.Layers.layers.ConvTranspose3d">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.Layers.layers.</code><code class="sig-name descname">ConvTranspose3d</code><span class="sig-paren">(</span><em class="sig-param">in_channels: int, out_channels: int, kernel_size: Union[int, Sequence[int]], stride: Union[int, Sequence[int]] = 1, padding: Union[Sequence[Union[Tuple[int, int], int]], int, str, TorchSnippet.typing_.PaddingMode] = &lt;PaddingMode.NONE: 'none'&gt;, output_padding: Union[int, Sequence[int]] = 0, dilation: Union[int, Sequence[int]] = 1, use_bias: Optional[bool] = None, activation: Union[Module, Type[Module], Callable[[], Module], None] = None, normalizer: Union[Module, Type[Module], Callable[[int], Module], None] = None, weight_norm: Union[bool, str, TorchSnippet.typing_.WeightNormMode] = False, gated: bool = False, gate_bias: float = 2.0, weight_init: Union[int, float, numpy.ndarray, Callable[[...], None]] = &lt;function kaming_uniform&gt;, bias_init: Union[int, float, numpy.ndarray, Callable[[...], None]] = &lt;function zeros&gt;, data_init: Union[Type[DataDependentInitializer], DataDependentInitializer, Callable[[...], DataDependentInitializer], None] = None</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.Layers.layers.ConvTranspose3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">TorchSnippet.Layers.layers.ConvTransposeNd</span></code></p>
</dd></dl>

</div>
<div class="section" id="module-TorchSnippet.Layers.utils">
<span id="torchsnippet-layers-utils-module"></span><h2>TorchSnippet.Layers.utils module<a class="headerlink" href="#module-TorchSnippet.Layers.utils" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="TorchSnippet.Layers.utils.flatten_nested_layers">
<code class="sig-prename descclassname">TorchSnippet.Layers.utils.</code><code class="sig-name descname">flatten_nested_layers</code><span class="sig-paren">(</span><em class="sig-param">nested_layers: Sequence[Union[torch.nn.modules.module.Module, Sequence[Union[torch.nn.modules.module.Module, Sequence[torch.nn.modules.module.Module]]]]]</em><span class="sig-paren">)</span> &#x2192; List[torch.nn.modules.module.Module]<a class="headerlink" href="#TorchSnippet.Layers.utils.flatten_nested_layers" title="Permalink to this definition">¶</a></dt>
<dd><p>Flatten a nested list of layers into a list of layers.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>nested_layers: Nested list of layers.</p>
</dd>
<dt>Returns:</dt><dd><p>The flatten layer list.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="TorchSnippet.Layers.utils.get_activation_class">
<code class="sig-prename descclassname">TorchSnippet.Layers.utils.</code><code class="sig-name descname">get_activation_class</code><span class="sig-paren">(</span><em class="sig-param">activation: Optional[str]</em><span class="sig-paren">)</span> &#x2192; Optional[Type[torch.nn.modules.module.Module]]<a class="headerlink" href="#TorchSnippet.Layers.utils.get_activation_class" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the activation module class according to <cite>name</cite>.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>activation: The activation name, or None (indicating no activation).</p>
</dd>
<dt>Returns:</dt><dd><p>The module class, or None (indicating no activation).</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="TorchSnippet.Layers.utils.get_deconv_output_padding">
<code class="sig-prename descclassname">TorchSnippet.Layers.utils.</code><code class="sig-name descname">get_deconv_output_padding</code><span class="sig-paren">(</span><em class="sig-param">input_size: List[int], output_size: List[int], kernel_size: Union[int, List[int]] = 1, stride: Union[int, List[int]] = 1, padding: Union[int, List[int], str, TorchSnippet.typing_.PaddingMode] = &lt;PaddingMode.NONE: 'none'&gt;, dilation: Union[int, List[int]] = 1</em><span class="sig-paren">)</span> &#x2192; List[int]<a class="headerlink" href="#TorchSnippet.Layers.utils.get_deconv_output_padding" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the <cite>output_padding</cite> for deconvolution (conv_transpose).</p>
<dl class="simple">
<dt>Args:</dt><dd><p>input_size: The input size (shape) of the spatial dimensions.
output_size: The output size (shape) of the spatial dimensions.
kernel_size: The kernel size.
stride: The stride.
padding: The padding.
dilation: The dilation.</p>
</dd>
<dt>Returns:</dt><dd><p>The output padding, can be used to construct a deconvolution
(conv transpose) layer.</p>
</dd>
<dt>Raises:</dt><dd><dl class="simple">
<dt>ValueError: If any argument is invalid, or no output padding</dt><dd><p>can satisfy the specified arguments.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-TorchSnippet.Layers">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-TorchSnippet.Layers" title="Permalink to this headline">¶</a></h2>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="TorchSnippet.ode.html" class="btn btn-neutral float-right" title="TorchSnippet.ode package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="TorchSnippet.Flow.html" class="btn btn-neutral float-left" title="TorchSnippet.Flow package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Tijin Yan

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>