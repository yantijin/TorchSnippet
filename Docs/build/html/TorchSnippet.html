

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>TorchSnippet package &mdash; TorchSnippet  documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="TorchSnippet.Flow package" href="TorchSnippet.Flow.html" />
    <link rel="prev" title="TorchSnippet" href="modules.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> TorchSnippet
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">TorchSnippet</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">TorchSnippet package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#subpackages">Subpackages</a><ul>
<li class="toctree-l4"><a class="reference internal" href="TorchSnippet.Flow.html">TorchSnippet.Flow package</a></li>
<li class="toctree-l4"><a class="reference internal" href="TorchSnippet.Layers.html">TorchSnippet.Layers package</a></li>
<li class="toctree-l4"><a class="reference internal" href="TorchSnippet.ode.html">TorchSnippet.ode package</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#submodules">Submodules</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-TorchSnippet.arg_check">TorchSnippet.arg_check module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-TorchSnippet.core">TorchSnippet.core module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-TorchSnippet.typing_">TorchSnippet.typing_ module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-TorchSnippet">Module contents</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="TorchSnippet.Layers.html">TorchSnippet.Layers package</a></li>
<li class="toctree-l1"><a class="reference internal" href="TorchSnippet.Flow.html">TorchSnippet.Flow package</a></li>
<li class="toctree-l1"><a class="reference internal" href="TorchSnippet.ode.html">TorchSnippet.ode package</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">TorchSnippet</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
          <li><a href="modules.html">TorchSnippet</a> &raquo;</li>
        
      <li>TorchSnippet package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/TorchSnippet.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="torchsnippet-package">
<h1>TorchSnippet package<a class="headerlink" href="#torchsnippet-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="subpackages">
<h2>Subpackages<a class="headerlink" href="#subpackages" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="TorchSnippet.Flow.html">TorchSnippet.Flow package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="TorchSnippet.Flow.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="TorchSnippet.Flow.html#module-TorchSnippet.Flow.actnorm">TorchSnippet.Flow.actnorm module</a></li>
<li class="toctree-l2"><a class="reference internal" href="TorchSnippet.Flow.html#module-TorchSnippet.Flow.base">TorchSnippet.Flow.base module</a></li>
<li class="toctree-l2"><a class="reference internal" href="TorchSnippet.Flow.html#module-TorchSnippet.Flow.coupling">TorchSnippet.Flow.coupling module</a></li>
<li class="toctree-l2"><a class="reference internal" href="TorchSnippet.Flow.html#module-TorchSnippet.Flow.invertible">TorchSnippet.Flow.invertible module</a></li>
<li class="toctree-l2"><a class="reference internal" href="TorchSnippet.Flow.html#module-TorchSnippet.Flow.nf">TorchSnippet.Flow.nf module</a></li>
<li class="toctree-l2"><a class="reference internal" href="TorchSnippet.Flow.html#module-TorchSnippet.Flow.reshape">TorchSnippet.Flow.reshape module</a></li>
<li class="toctree-l2"><a class="reference internal" href="TorchSnippet.Flow.html#module-TorchSnippet.Flow.scale">TorchSnippet.Flow.scale module</a></li>
<li class="toctree-l2"><a class="reference internal" href="TorchSnippet.Flow.html#module-TorchSnippet.Flow.shuffle">TorchSnippet.Flow.shuffle module</a></li>
<li class="toctree-l2"><a class="reference internal" href="TorchSnippet.Flow.html#module-TorchSnippet.Flow.utils">TorchSnippet.Flow.utils module</a></li>
<li class="toctree-l2"><a class="reference internal" href="TorchSnippet.Flow.html#module-TorchSnippet.Flow">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="TorchSnippet.Layers.html">TorchSnippet.Layers package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="TorchSnippet.Layers.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="TorchSnippet.Layers.html#module-TorchSnippet.Layers.activation">TorchSnippet.Layers.activation module</a></li>
<li class="toctree-l2"><a class="reference internal" href="TorchSnippet.Layers.html#module-TorchSnippet.Layers.base">TorchSnippet.Layers.base module</a></li>
<li class="toctree-l2"><a class="reference internal" href="TorchSnippet.Layers.html#module-TorchSnippet.Layers.gated">TorchSnippet.Layers.gated module</a></li>
<li class="toctree-l2"><a class="reference internal" href="TorchSnippet.Layers.html#module-TorchSnippet.Layers.initializer">TorchSnippet.Layers.initializer module</a></li>
<li class="toctree-l2"><a class="reference internal" href="TorchSnippet.Layers.html#module-TorchSnippet.Layers.layers">TorchSnippet.Layers.layers module</a></li>
<li class="toctree-l2"><a class="reference internal" href="TorchSnippet.Layers.html#module-TorchSnippet.Layers.utils">TorchSnippet.Layers.utils module</a></li>
<li class="toctree-l2"><a class="reference internal" href="TorchSnippet.Layers.html#module-TorchSnippet.Layers">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="TorchSnippet.ode.html">TorchSnippet.ode package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="TorchSnippet.ode.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="TorchSnippet.ode.html#module-TorchSnippet.ode.adjoint">TorchSnippet.ode.adjoint module</a></li>
<li class="toctree-l2"><a class="reference internal" href="TorchSnippet.ode.html#module-TorchSnippet.ode.misc">TorchSnippet.ode.misc module</a></li>
<li class="toctree-l2"><a class="reference internal" href="TorchSnippet.ode.html#module-TorchSnippet.ode.odefunc">TorchSnippet.ode.odefunc module</a></li>
<li class="toctree-l2"><a class="reference internal" href="TorchSnippet.ode.html#module-TorchSnippet.ode.odeint">TorchSnippet.ode.odeint module</a></li>
<li class="toctree-l2"><a class="reference internal" href="TorchSnippet.ode.html#module-TorchSnippet.ode.rk_common">TorchSnippet.ode.rk_common module</a></li>
<li class="toctree-l2"><a class="reference internal" href="TorchSnippet.ode.html#module-TorchSnippet.ode.solvers">TorchSnippet.ode.solvers module</a></li>
<li class="toctree-l2"><a class="reference internal" href="TorchSnippet.ode.html#module-TorchSnippet.ode">Module contents</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-TorchSnippet.arg_check">
<span id="torchsnippet-arg-check-module"></span><h2>TorchSnippet.arg_check module<a class="headerlink" href="#module-TorchSnippet.arg_check" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="TorchSnippet.arg_check.validate_conv_size">
<code class="sig-prename descclassname">TorchSnippet.arg_check.</code><code class="sig-name descname">validate_conv_size</code><span class="sig-paren">(</span><em class="sig-param">name: str, value: Union[int, Sequence[int]], spatial_ndims: int</em><span class="sig-paren">)</span> &#x2192; List[int]<a class="headerlink" href="#TorchSnippet.arg_check.validate_conv_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.arg_check.validate_positive_int">
<code class="sig-prename descclassname">TorchSnippet.arg_check.</code><code class="sig-name descname">validate_positive_int</code><span class="sig-paren">(</span><em class="sig-param">arg_name: str</em>, <em class="sig-param">arg_value</em><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#TorchSnippet.arg_check.validate_positive_int" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.arg_check.validate_padding">
<code class="sig-prename descclassname">TorchSnippet.arg_check.</code><code class="sig-name descname">validate_padding</code><span class="sig-paren">(</span><em class="sig-param">padding: Union[Sequence[Union[Tuple[int, int], int]], int, str, TorchSnippet.typing_.PaddingMode], kernel_size: List[int], dilation: List[int], spatial_ndims: int</em><span class="sig-paren">)</span> &#x2192; List[Tuple[int, int]]<a class="headerlink" href="#TorchSnippet.arg_check.validate_padding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.arg_check.maybe_as_symmetric_padding">
<code class="sig-prename descclassname">TorchSnippet.arg_check.</code><code class="sig-name descname">maybe_as_symmetric_padding</code><span class="sig-paren">(</span><em class="sig-param">padding: List[Tuple[int, int]]</em><span class="sig-paren">)</span> &#x2192; Optional[List[int]]<a class="headerlink" href="#TorchSnippet.arg_check.maybe_as_symmetric_padding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.arg_check.validate_output_padding">
<code class="sig-prename descclassname">TorchSnippet.arg_check.</code><code class="sig-name descname">validate_output_padding</code><span class="sig-paren">(</span><em class="sig-param">output_padding: Union[int, Sequence[int]], stride: List[int], dilation: List[int], spatial_ndims: int</em><span class="sig-paren">)</span> &#x2192; List[int]<a class="headerlink" href="#TorchSnippet.arg_check.validate_output_padding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.arg_check.get_layer_from_layer_or_factory">
<code class="sig-prename descclassname">TorchSnippet.arg_check.</code><code class="sig-name descname">get_layer_from_layer_or_factory</code><span class="sig-paren">(</span><em class="sig-param">arg_name: str</em>, <em class="sig-param">layer_or_layer_factory</em>, <em class="sig-param">args=()</em>, <em class="sig-param">kwargs=None</em><span class="sig-paren">)</span> &#x2192; torch.nn.modules.module.Module<a class="headerlink" href="#TorchSnippet.arg_check.get_layer_from_layer_or_factory" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.arg_check.is_finite">
<code class="sig-prename descclassname">TorchSnippet.arg_check.</code><code class="sig-name descname">is_finite</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#TorchSnippet.arg_check.is_finite" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.arg_check.is_all">
<code class="sig-prename descclassname">TorchSnippet.arg_check.</code><code class="sig-name descname">is_all</code><span class="sig-paren">(</span><em class="sig-param">condition: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; bool<a class="headerlink" href="#TorchSnippet.arg_check.is_all" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.arg_check.assert_finite">
<code class="sig-prename descclassname">TorchSnippet.arg_check.</code><code class="sig-name descname">assert_finite</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">message: str</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#TorchSnippet.arg_check.assert_finite" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-TorchSnippet.core">
<span id="torchsnippet-core-module"></span><h2>TorchSnippet.core module<a class="headerlink" href="#module-TorchSnippet.core" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="TorchSnippet.core.add_parameter">
<code class="sig-prename descclassname">TorchSnippet.core.</code><code class="sig-name descname">add_parameter</code><span class="sig-paren">(</span><em class="sig-param">layer: torch.nn.modules.module.Module, name: str, value: Optional[torch.Tensor], requires_grad: bool = True</em><span class="sig-paren">)</span> &#x2192; Optional[torch.Tensor]<a class="headerlink" href="#TorchSnippet.core.add_parameter" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.core.get_parameter">
<code class="sig-prename descclassname">TorchSnippet.core.</code><code class="sig-name descname">get_parameter</code><span class="sig-paren">(</span><em class="sig-param">layer: torch.nn.modules.module.Module</em>, <em class="sig-param">name: str</em><span class="sig-paren">)</span> &#x2192; Optional[torch.Tensor]<a class="headerlink" href="#TorchSnippet.core.get_parameter" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.core.get_parameters">
<code class="sig-prename descclassname">TorchSnippet.core.</code><code class="sig-name descname">get_parameters</code><span class="sig-paren">(</span><em class="sig-param">layer: torch.nn.modules.module.Module</em>, <em class="sig-param">recursive: bool = True</em><span class="sig-paren">)</span> &#x2192; Iterator[Tuple[str, torch.Tensor]]<a class="headerlink" href="#TorchSnippet.core.get_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.core.add_buffer">
<code class="sig-prename descclassname">TorchSnippet.core.</code><code class="sig-name descname">add_buffer</code><span class="sig-paren">(</span><em class="sig-param">layer: torch.nn.modules.module.Module, name: str, value: Optional[torch.Tensor]</em><span class="sig-paren">)</span> &#x2192; Optional[torch.Tensor]<a class="headerlink" href="#TorchSnippet.core.add_buffer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.core.get_buffer">
<code class="sig-prename descclassname">TorchSnippet.core.</code><code class="sig-name descname">get_buffer</code><span class="sig-paren">(</span><em class="sig-param">layer: torch.nn.modules.module.Module</em>, <em class="sig-param">name: str</em><span class="sig-paren">)</span> &#x2192; Optional[torch.Tensor]<a class="headerlink" href="#TorchSnippet.core.get_buffer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.core.get_buffers">
<code class="sig-prename descclassname">TorchSnippet.core.</code><code class="sig-name descname">get_buffers</code><span class="sig-paren">(</span><em class="sig-param">layer: torch.nn.modules.module.Module</em>, <em class="sig-param">recursive: bool = True</em><span class="sig-paren">)</span> &#x2192; Iterator[Tuple[str, torch.Tensor]]<a class="headerlink" href="#TorchSnippet.core.get_buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.core.set_train_mode">
<code class="sig-prename descclassname">TorchSnippet.core.</code><code class="sig-name descname">set_train_mode</code><span class="sig-paren">(</span><em class="sig-param">layer: torch.nn.modules.module.Module</em>, <em class="sig-param">training: bool = True</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.core.set_train_mode" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.core.pad">
<code class="sig-prename descclassname">TorchSnippet.core.</code><code class="sig-name descname">pad</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor, padding: List[Tuple[int, int]], value: float = 0.0</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#TorchSnippet.core.pad" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.core.index_select">
<code class="sig-prename descclassname">TorchSnippet.core.</code><code class="sig-name descname">index_select</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">indices: torch.Tensor</em>, <em class="sig-param">axis: int</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#TorchSnippet.core.index_select" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.core.to_numpy">
<code class="sig-prename descclassname">TorchSnippet.core.</code><code class="sig-name descname">to_numpy</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; numpy.ndarray<a class="headerlink" href="#TorchSnippet.core.to_numpy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.core.variable">
<code class="sig-prename descclassname">TorchSnippet.core.</code><code class="sig-name descname">variable</code><span class="sig-paren">(</span><em class="sig-param">shape: List[int], dtype: Union[str, torch.dtype] = torch.float32, initializer: Union[int, float, numpy.ndarray, torch.Tensor, Callable[[torch.Tensor], None], None] = None, requires_grad: bool = True, force_copy: bool = True</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#TorchSnippet.core.variable" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a new variable.</p>
<dl>
<dt>Args:</dt><dd><p>shape: Shape of the variable.
dtype: Dtype of the variable.
initializer: The variable initializer.  It may be a scalar (which</p>
<blockquote>
<div><p>will be filled into the new variable), an array or another
<cite>Tensor</cite> with the same shape as specified <cite>shape</cite>, or a callable
function that can be used to initialize the variable.</p>
</div></blockquote>
<dl class="simple">
<dt>requires_grad: Whether or not that the variable requires gradient</dt><dd><p>during back-propagation?  Defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>.</p>
</dd>
<dt>force_copy: Whether or not to force copy the data from <cite>initializer</cite>,</dt><dd><p>even if the backend supports sharing memory?
Defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>The created variable.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="TorchSnippet.core.squeeze">
<code class="sig-prename descclassname">TorchSnippet.core.</code><code class="sig-name descname">squeeze</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor, axis: Union[int, List[int], None] = None</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#TorchSnippet.core.squeeze" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.core.flatten_to_ndims">
<code class="sig-prename descclassname">TorchSnippet.core.</code><code class="sig-name descname">flatten_to_ndims</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">ndims: int</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, Optional[List[int]]]<a class="headerlink" href="#TorchSnippet.core.flatten_to_ndims" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.core.unflatten_from_ndims">
<code class="sig-prename descclassname">TorchSnippet.core.</code><code class="sig-name descname">unflatten_from_ndims</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor, front_shape: Optional[List[int]]</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#TorchSnippet.core.unflatten_from_ndims" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.core.broadcast_to">
<code class="sig-prename descclassname">TorchSnippet.core.</code><code class="sig-name descname">broadcast_to</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor, new_shape: List[int]</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#TorchSnippet.core.broadcast_to" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.core.broadcast_shape">
<code class="sig-prename descclassname">TorchSnippet.core.</code><code class="sig-name descname">broadcast_shape</code><span class="sig-paren">(</span><em class="sig-param">x: List[int], y: List[int]</em><span class="sig-paren">)</span> &#x2192; List[int]<a class="headerlink" href="#TorchSnippet.core.broadcast_shape" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.core.get_dtype">
<code class="sig-prename descclassname">TorchSnippet.core.</code><code class="sig-name descname">get_dtype</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; str<a class="headerlink" href="#TorchSnippet.core.get_dtype" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.core.reduce_sum">
<code class="sig-prename descclassname">TorchSnippet.core.</code><code class="sig-name descname">reduce_sum</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor, axis: Union[int, List[int], None] = None, keepdims: bool = False</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#TorchSnippet.core.reduce_sum" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.core.reduce_max">
<code class="sig-prename descclassname">TorchSnippet.core.</code><code class="sig-name descname">reduce_max</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor, axis: Union[int, List[int], None] = None, keepdims: bool = False</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#TorchSnippet.core.reduce_max" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.core.reduce_mean">
<code class="sig-prename descclassname">TorchSnippet.core.</code><code class="sig-name descname">reduce_mean</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor, axis: Union[int, List[int], None] = None, keepdims: bool = False</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#TorchSnippet.core.reduce_mean" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.core.reduce_min">
<code class="sig-prename descclassname">TorchSnippet.core.</code><code class="sig-name descname">reduce_min</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor, axis: Union[int, List[int], None] = None, keepdims: bool = False</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#TorchSnippet.core.reduce_min" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.core.log_mean_exp">
<code class="sig-prename descclassname">TorchSnippet.core.</code><code class="sig-name descname">log_mean_exp</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor, axis: Union[int, List[int], None] = None, keepdims: bool = False</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#TorchSnippet.core.log_mean_exp" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.core.log_sum_exp">
<code class="sig-prename descclassname">TorchSnippet.core.</code><code class="sig-name descname">log_sum_exp</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor, axis: Union[int, List[int], None] = None, keepdims: bool = False</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#TorchSnippet.core.log_sum_exp" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.core.norm_except_axis">
<code class="sig-prename descclassname">TorchSnippet.core.</code><code class="sig-name descname">norm_except_axis</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor, axis: Union[int, List[int], None], p: float = 2, keepdims: bool = False</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#TorchSnippet.core.norm_except_axis" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the Lp-norm of a tensor except for specified axis.</p>
<dl>
<dt>Args:</dt><dd><p>input: The input tensor.
axis: The axis to keep for computing Lp-norm.</p>
<blockquote>
<div><p>All other axis will be reduced.  Defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>,
where no axis will be kept.</p>
</div></blockquote>
<p>p: The <cite>p</cite> of the <cite>Lp</cite> norm.  Defaults to 2.
keepdims: Whether or not to keep the reduced dimensions?</p>
<blockquote>
<div><p>Defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>.</p>
</div></blockquote>
</dd>
<dt>Returns:</dt><dd><p>The Lp-norm of the tensor.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="TorchSnippet.core.fill">
<code class="sig-prename descclassname">TorchSnippet.core.</code><code class="sig-name descname">fill</code><span class="sig-paren">(</span><em class="sig-param">dst: torch.Tensor</em>, <em class="sig-param">fill_value: float</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#TorchSnippet.core.fill" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.core.fill_zeros">
<code class="sig-prename descclassname">TorchSnippet.core.</code><code class="sig-name descname">fill_zeros</code><span class="sig-paren">(</span><em class="sig-param">dst: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#TorchSnippet.core.fill_zeros" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.core.assign_data">
<code class="sig-prename descclassname">TorchSnippet.core.</code><code class="sig-name descname">assign_data</code><span class="sig-paren">(</span><em class="sig-param">dst: torch.Tensor</em>, <em class="sig-param">src</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#TorchSnippet.core.assign_data" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.core.zeros">
<code class="sig-prename descclassname">TorchSnippet.core.</code><code class="sig-name descname">zeros</code><span class="sig-paren">(</span><em class="sig-param">shape: List[int], dtype: Union[str, torch.dtype] = 'float32'</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#TorchSnippet.core.zeros" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="TorchSnippet.core.as_tensor">
<code class="sig-prename descclassname">TorchSnippet.core.</code><code class="sig-name descname">as_tensor</code><span class="sig-paren">(</span><em class="sig-param">data</em>, <em class="sig-param">dtype: Union[torch.dtype</em>, <em class="sig-param">str</em>, <em class="sig-param">None] = None</em>, <em class="sig-param">force_copy: bool = False</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#TorchSnippet.core.as_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Construct a new tensor from <cite>data</cite>.</p>
<p>This method will copy <cite>data</cite> only when it is required to do so, or
when <cite>force_copy</cite> is set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>.</p>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>data: The tensor data.  It might be a Python number, a NumPy array,</dt><dd><p>another tensor, a <code class="xref py py-class docutils literal notranslate"><span class="pre">StochasticTensor</span></code>, or anything
else that the backend supports.</p>
</dd>
</dl>
<p>dtype: The expected dtype of the constructed tensor.
force_copy: Force to copy <cite>data</cite> even if it is not necessary.</p>
<blockquote>
<div><p>The gradient propagation will not be stopped from the copied tensor
to the original tensor.  The caller may need to use <cite>T.stop_grad()</cite>
if necessary.</p>
<p>It should not be necessary to copy the given <cite>data</cite>, if <cite>data</cite>
is already another tensor with <cite>dtype</cite>; or if <cite>data</cite> is a NumPy
array with compatible <cite>dtype</cite>, and the backend supports to share
memory between a tensor and a NumPy array.</p>
</div></blockquote>
</dd>
<dt>Returns:</dt><dd><p>The constructed tensor.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="TorchSnippet.core.calculate_mean_and_var">
<code class="sig-prename descclassname">TorchSnippet.core.</code><code class="sig-name descname">calculate_mean_and_var</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">axis: Optional[List[int]] = None</em>, <em class="sig-param">keepdims: bool = False</em>, <em class="sig-param">unbiased: bool = True</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="headerlink" href="#TorchSnippet.core.calculate_mean_and_var" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-TorchSnippet.typing_">
<span id="torchsnippet-typing-module"></span><h2>TorchSnippet.typing_ module<a class="headerlink" href="#module-TorchSnippet.typing_" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="TorchSnippet.typing_.Tensor">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.typing_.</code><code class="sig-name descname">Tensor</code><a class="headerlink" href="#TorchSnippet.typing_.Tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch._C._TensorBase</span></code></p>
<dl class="method">
<dt id="TorchSnippet.typing_.Tensor.align_to">
<code class="sig-name descname">align_to</code><span class="sig-paren">(</span><em class="sig-param">*names</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Tensor.align_to" title="Permalink to this definition">¶</a></dt>
<dd><p>Permutes the dimensions of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor to match the order
specified in <code class="xref py py-attr docutils literal notranslate"><span class="pre">names</span></code>, adding size-one dims for any new names.</p>
<p>All of the dims of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> must be named in order to use this method.
The resulting tensor is a view on the original tensor.</p>
<p>All dimension names of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> must be present in <code class="xref py py-attr docutils literal notranslate"><span class="pre">names</span></code>.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">names</span></code> may contain additional names that are not in <code class="docutils literal notranslate"><span class="pre">self.names</span></code>;
the output tensor has a size-one dimension for each of those new names.</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">names</span></code> may contain up to one Ellipsis (<code class="docutils literal notranslate"><span class="pre">...</span></code>).
The Ellipsis is expanded to be equal to all dimension names of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>
that are not mentioned in <code class="xref py py-attr docutils literal notranslate"><span class="pre">names</span></code>, in the order that they appear
in <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
<p>Python 2 does not support Ellipsis but one may use a string literal
instead (<code class="docutils literal notranslate"><span class="pre">'...'</span></code>).</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>names (iterable of str): The desired dimension ordering of the</dt><dd><p>output tensor. May contain up to one Ellipsis that is expanded
to all unmentioned dim names of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">named_tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">refine_names</span><span class="p">(</span><span class="s1">&#39;A&#39;</span><span class="p">,</span> <span class="s1">&#39;B&#39;</span><span class="p">,</span> <span class="s1">&#39;C&#39;</span><span class="p">,</span> <span class="s1">&#39;D&#39;</span><span class="p">,</span> <span class="s1">&#39;E&#39;</span><span class="p">,</span> <span class="s1">&#39;F&#39;</span><span class="p">)</span>

<span class="go"># Move the F and E dims to the front while keeping the rest in order</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">named_tensor</span><span class="o">.</span><span class="n">align_to</span><span class="p">(</span><span class="s1">&#39;F&#39;</span><span class="p">,</span> <span class="s1">&#39;E&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The named tensor API is experimental and subject to change.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Tensor.backward">
<code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">gradient=None</em>, <em class="sig-param">retain_graph=None</em>, <em class="sig-param">create_graph=False</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Tensor.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the gradient of current tensor w.r.t. graph leaves.</p>
<p>The graph is differentiated using the chain rule. If the tensor is
non-scalar (i.e. its data has more than one element) and requires
gradient, the function additionally requires specifying <code class="docutils literal notranslate"><span class="pre">gradient</span></code>.
It should be a tensor of matching type and location, that contains
the gradient of the differentiated function w.r.t. <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
<p>This function accumulates gradients in the leaves - you might need to
zero them before calling it.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>gradient (Tensor or None): Gradient w.r.t. the</dt><dd><p>tensor. If it is a tensor, it will be automatically converted
to a Tensor that does not require grad unless <code class="docutils literal notranslate"><span class="pre">create_graph</span></code> is True.
None values can be specified for scalar Tensors or ones that
don’t require grad. If a None value would be acceptable then
this argument is optional.</p>
</dd>
<dt>retain_graph (bool, optional): If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the graph used to compute</dt><dd><p>the grads will be freed. Note that in nearly all cases setting
this option to True is not needed and often can be worked around
in a much more efficient way. Defaults to the value of
<code class="docutils literal notranslate"><span class="pre">create_graph</span></code>.</p>
</dd>
<dt>create_graph (bool, optional): If <code class="docutils literal notranslate"><span class="pre">True</span></code>, graph of the derivative will</dt><dd><p>be constructed, allowing to compute higher order derivative
products. Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Tensor.detach">
<code class="sig-name descname">detach</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Tensor.detach" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new Tensor, detached from the current graph.</p>
<p>The result will never require gradient.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Returned Tensor shares the same storage with the original one.
In-place modifications on either of them will be seen, and may trigger
errors in correctness checks.
IMPORTANT NOTE: Previously, in-place size / stride / storage changes
(such as <cite>resize_</cite> / <cite>resize_as_</cite> / <cite>set_</cite> / <cite>transpose_</cite>) to the returned tensor
also update the original tensor. Now, these in-place changes will not update the
original tensor anymore, and will instead trigger an error.
For sparse tensors:
In-place indices / values changes (such as <cite>zero_</cite> / <cite>copy_</cite> / <cite>add_</cite>) to the
returned tensor will not update the original tensor anymore, and will instead
trigger an error.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Tensor.detach_">
<code class="sig-name descname">detach_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Tensor.detach_" title="Permalink to this definition">¶</a></dt>
<dd><p>Detaches the Tensor from the graph that created it, making it a leaf.
Views cannot be detached in-place.</p>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Tensor.is_shared">
<code class="sig-name descname">is_shared</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Tensor.is_shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Checks if tensor is in shared memory.</p>
<p>This is always <code class="docutils literal notranslate"><span class="pre">True</span></code> for CUDA tensors.</p>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Tensor.lu">
<code class="sig-name descname">lu</code><span class="sig-paren">(</span><em class="sig-param">pivot=True</em>, <em class="sig-param">get_infos=False</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Tensor.lu" title="Permalink to this definition">¶</a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lu()</span></code></p>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Tensor.norm">
<code class="sig-name descname">norm</code><span class="sig-paren">(</span><em class="sig-param">p='fro'</em>, <em class="sig-param">dim=None</em>, <em class="sig-param">keepdim=False</em>, <em class="sig-param">dtype=None</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Tensor.norm" title="Permalink to this definition">¶</a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.norm()</span></code></p>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Tensor.refine_names">
<code class="sig-name descname">refine_names</code><span class="sig-paren">(</span><em class="sig-param">*names</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Tensor.refine_names" title="Permalink to this definition">¶</a></dt>
<dd><p>Refines the dimension names of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> according to <code class="xref py py-attr docutils literal notranslate"><span class="pre">names</span></code>.</p>
<p>Refining is a special case of renaming that “lifts” unnamed dimensions.
A <code class="docutils literal notranslate"><span class="pre">None</span></code> dim can be refined to have any name; a named dim can only be
refined to have the same name.</p>
<p>Because named tensors can coexist with unnamed tensors, refining names
gives a nice way to write named-tensor-aware code that works with both
named and unnamed tensors.</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">names</span></code> may contain up to one Ellipsis (<code class="docutils literal notranslate"><span class="pre">...</span></code>).
The Ellipsis is expanded greedily; it is expanded in-place to fill
<code class="xref py py-attr docutils literal notranslate"><span class="pre">names</span></code> to the same length as <code class="docutils literal notranslate"><span class="pre">self.dim()</span></code> using names from the
corresponding indices of <code class="docutils literal notranslate"><span class="pre">self.names</span></code>.</p>
<p>Python 2 does not support Ellipsis but one may use a string literal
instead (<code class="docutils literal notranslate"><span class="pre">'...'</span></code>).</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>names (iterable of str): The desired names of the output tensor. May</dt><dd><p>contain up to one Ellipsis.</p>
</dd>
</dl>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">imgs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">named_imgs</span> <span class="o">=</span> <span class="n">imgs</span><span class="o">.</span><span class="n">refine_names</span><span class="p">(</span><span class="s1">&#39;N&#39;</span><span class="p">,</span> <span class="s1">&#39;C&#39;</span><span class="p">,</span> <span class="s1">&#39;H&#39;</span><span class="p">,</span> <span class="s1">&#39;W&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">named_imgs</span><span class="o">.</span><span class="n">names</span>
<span class="go">(&#39;N&#39;, &#39;C&#39;, &#39;H&#39;, &#39;W&#39;)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">refine_names</span><span class="p">(</span><span class="s1">&#39;A&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="s1">&#39;B&#39;</span><span class="p">,</span> <span class="s1">&#39;C&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">names</span>
<span class="go">(&#39;A&#39;, None, None, &#39;B&#39;, &#39;C&#39;)</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The named tensor API is experimental and subject to change.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Tensor.register_hook">
<code class="sig-name descname">register_hook</code><span class="sig-paren">(</span><em class="sig-param">hook</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Tensor.register_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a backward hook.</p>
<p>The hook will be called every time a gradient with respect to the
Tensor is computed. The hook should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span> <span class="ow">or</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The hook should not modify its argument, but it can optionally return
a new gradient which will be used in place of <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad</span></code>.</p>
<p>This function returns a handle with a method <code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code>
that removes the hook from the module.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span><span class="k">lambda</span> <span class="n">grad</span><span class="p">:</span> <span class="n">grad</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># double the gradient</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span><span class="o">.</span><span class="n">grad</span>

<span class="go"> 2</span>
<span class="go"> 4</span>
<span class="go"> 6</span>
<span class="go">[torch.FloatTensor of size (3,)]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">h</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>  <span class="c1"># removes the hook</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Tensor.reinforce">
<code class="sig-name descname">reinforce</code><span class="sig-paren">(</span><em class="sig-param">reward</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Tensor.reinforce" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Tensor.rename">
<code class="sig-name descname">rename</code><span class="sig-paren">(</span><em class="sig-param">*names</em>, <em class="sig-param">**rename_map</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Tensor.rename" title="Permalink to this definition">¶</a></dt>
<dd><p>Renames dimension names of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
<p>There are two main usages:</p>
<p><code class="docutils literal notranslate"><span class="pre">self.rename(**rename_map)</span></code> returns a view on tensor that has dims
renamed as specified in the mapping <code class="xref py py-attr docutils literal notranslate"><span class="pre">rename_map</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">self.rename(*names)</span></code> returns a view on tensor, renaming all
dimensions positionally using <code class="xref py py-attr docutils literal notranslate"><span class="pre">names</span></code>.
Use <code class="docutils literal notranslate"><span class="pre">self.rename(None)</span></code> to drop names on a tensor.</p>
<p>One cannot specify both positional args <code class="xref py py-attr docutils literal notranslate"><span class="pre">names</span></code> and keyword args
<code class="xref py py-attr docutils literal notranslate"><span class="pre">rename_map</span></code>.</p>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">imgs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;N&#39;</span><span class="p">,</span> <span class="s1">&#39;C&#39;</span><span class="p">,</span> <span class="s1">&#39;H&#39;</span><span class="p">,</span> <span class="s1">&#39;W&#39;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">renamed_imgs</span> <span class="o">=</span> <span class="n">imgs</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">N</span><span class="o">=</span><span class="s1">&#39;batch&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="s1">&#39;channels&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">renamed_imgs</span><span class="o">.</span><span class="n">names</span>
<span class="go">(&#39;batch&#39;, &#39;channels&#39;, &#39;H&#39;, &#39;W&#39;)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">renamed_imgs</span> <span class="o">=</span> <span class="n">imgs</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">renamed_imgs</span><span class="o">.</span><span class="n">names</span>
<span class="go">(None,)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">renamed_imgs</span> <span class="o">=</span> <span class="n">imgs</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="s1">&#39;batch&#39;</span><span class="p">,</span> <span class="s1">&#39;channel&#39;</span><span class="p">,</span> <span class="s1">&#39;height&#39;</span><span class="p">,</span> <span class="s1">&#39;width&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">renamed_imgs</span><span class="o">.</span><span class="n">names</span>
<span class="go">(&#39;batch&#39;, &#39;channel&#39;, &#39;height&#39;, &#39;width&#39;)</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The named tensor API is experimental and subject to change.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Tensor.rename_">
<code class="sig-name descname">rename_</code><span class="sig-paren">(</span><em class="sig-param">*names</em>, <em class="sig-param">**rename_map</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Tensor.rename_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#TorchSnippet.typing_.Tensor.rename" title="TorchSnippet.typing_.Tensor.rename"><code class="xref py py-meth docutils literal notranslate"><span class="pre">rename()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Tensor.resize">
<code class="sig-name descname">resize</code><span class="sig-paren">(</span><em class="sig-param">*sizes</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Tensor.resize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Tensor.resize_as">
<code class="sig-name descname">resize_as</code><span class="sig-paren">(</span><em class="sig-param">tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Tensor.resize_as" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Tensor.retain_grad">
<code class="sig-name descname">retain_grad</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Tensor.retain_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Enables .grad attribute for non-leaf Tensors.</p>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Tensor.share_memory_">
<code class="sig-name descname">share_memory_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Tensor.share_memory_" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves the underlying storage to shared memory.</p>
<p>This is a no-op if the underlying storage is already in shared memory
and for CUDA tensors. Tensors in shared memory cannot be resized.</p>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Tensor.split">
<code class="sig-name descname">split</code><span class="sig-paren">(</span><em class="sig-param">split_size</em>, <em class="sig-param">dim=0</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Tensor.split" title="Permalink to this definition">¶</a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.split()</span></code></p>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Tensor.stft">
<code class="sig-name descname">stft</code><span class="sig-paren">(</span><em class="sig-param">n_fft</em>, <em class="sig-param">hop_length=None</em>, <em class="sig-param">win_length=None</em>, <em class="sig-param">window=None</em>, <em class="sig-param">center=True</em>, <em class="sig-param">pad_mode='reflect'</em>, <em class="sig-param">normalized=False</em>, <em class="sig-param">onesided=True</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Tensor.stft" title="Permalink to this definition">¶</a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.stft()</span></code></p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This function changed signature at version 0.4.1. Calling with
the previous signature may cause error or return incorrect result.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Tensor.unflatten">
<code class="sig-name descname">unflatten</code><span class="sig-paren">(</span><em class="sig-param">dim</em>, <em class="sig-param">namedshape</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Tensor.unflatten" title="Permalink to this definition">¶</a></dt>
<dd><p>Unflattens the named dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>, viewing it in the shape
specified by <code class="xref py py-attr docutils literal notranslate"><span class="pre">namedshape</span></code>.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>namedshape: (iterable of <code class="docutils literal notranslate"><span class="pre">(name,</span> <span class="pre">size)</span></code> tuples).</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">flat_imgs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="mi">128</span> <span class="o">*</span> <span class="mi">128</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;N&#39;</span><span class="p">,</span> <span class="s1">&#39;features&#39;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">imgs</span> <span class="o">=</span> <span class="n">flat_imgs</span><span class="o">.</span><span class="n">unflatten</span><span class="p">(</span><span class="s1">&#39;features&#39;</span><span class="p">,</span> <span class="p">((</span><span class="s1">&#39;C&#39;</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;H&#39;</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="mi">128</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">imgs</span><span class="o">.</span><span class="n">names</span><span class="p">,</span> <span class="n">images</span><span class="o">.</span><span class="n">shape</span>
<span class="go">((&#39;N&#39;, &#39;C&#39;, &#39;H&#39;, &#39;W&#39;), torch.Size([32, 3, 128, 128]))</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The named tensor API is experimental and subject to change.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Tensor.unique">
<code class="sig-name descname">unique</code><span class="sig-paren">(</span><em class="sig-param">sorted=True</em>, <em class="sig-param">return_inverse=False</em>, <em class="sig-param">return_counts=False</em>, <em class="sig-param">dim=None</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Tensor.unique" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the unique elements of the input tensor.</p>
<p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.unique()</span></code></p>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Tensor.unique_consecutive">
<code class="sig-name descname">unique_consecutive</code><span class="sig-paren">(</span><em class="sig-param">return_inverse=False</em>, <em class="sig-param">return_counts=False</em>, <em class="sig-param">dim=None</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Tensor.unique_consecutive" title="Permalink to this definition">¶</a></dt>
<dd><p>Eliminates all but the first element from every consecutive group of equivalent elements.</p>
<p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.unique_consecutive()</span></code></p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="TorchSnippet.typing_.Module">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.typing_.</code><code class="sig-name descname">Module</code><a class="headerlink" href="#TorchSnippet.typing_.Module" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Model</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call <a class="reference internal" href="#TorchSnippet.typing_.Module.to" title="TorchSnippet.typing_.Module.to"><code class="xref py py-meth docutils literal notranslate"><span class="pre">to()</span></code></a>, etc.</p>
<dl class="method">
<dt id="TorchSnippet.typing_.Module.add_module">
<code class="sig-name descname">add_module</code><span class="sig-paren">(</span><em class="sig-param">name</em>, <em class="sig-param">module</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Module.add_module" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a child module to the current module.</p>
<p>The module can be accessed as an attribute using the given name.</p>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>name (string): name of the child module. The child module can be</dt><dd><p>accessed from this module using the given name</p>
</dd>
</dl>
<p>module (Module): child module to be added to the module.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Module.apply">
<code class="sig-name descname">apply</code><span class="sig-paren">(</span><em class="sig-param">fn</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Module.apply" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies <code class="docutils literal notranslate"><span class="pre">fn</span></code> recursively to every submodule (as returned by <code class="docutils literal notranslate"><span class="pre">.children()</span></code>)
as well as self. Typical use includes initializing the parameters of a model
(see also <span class="xref std std-ref">nn-init-doc</span>).</p>
<dl class="simple">
<dt>Args:</dt><dd><p>fn (<a class="reference internal" href="#TorchSnippet.typing_.Module" title="TorchSnippet.typing_.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> -&gt; None): function to be applied to each submodule</p>
</dd>
<dt>Returns:</dt><dd><p>Module: self</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="o">==</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">init_weights</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 1.,  1.],</span>
<span class="go">        [ 1.,  1.]])</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 1.,  1.],</span>
<span class="go">        [ 1.,  1.]])</span>
<span class="go">Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
<span class="go">Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Module.buffers">
<code class="sig-name descname">buffers</code><span class="sig-paren">(</span><em class="sig-param">recurse=True</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Module.buffers" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module buffers.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>recurse (bool): if True, then yields buffers of this module</dt><dd><p>and all submodules. Otherwise, yields only buffers that
are direct members of this module.</p>
</dd>
</dl>
</dd>
<dt>Yields:</dt><dd><p>torch.Tensor: module buffer</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">buf</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">buffers</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">data</span><span class="p">),</span> <span class="n">buf</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">&lt;class &#39;torch.FloatTensor&#39;&gt; (20L,)</span>
<span class="go">&lt;class &#39;torch.FloatTensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Module.children">
<code class="sig-name descname">children</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Module.children" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over immediate children modules.</p>
<dl class="simple">
<dt>Yields:</dt><dd><p>Module: a child module</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Module.cpu">
<code class="sig-name descname">cpu</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Module.cpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves all model parameters and buffers to the CPU.</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Module: self</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Module.cuda">
<code class="sig-name descname">cuda</code><span class="sig-paren">(</span><em class="sig-param">device=None</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Module.cuda" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves all model parameters and buffers to the GPU.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on GPU while being optimized.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>device (int, optional): if specified, all parameters will be</dt><dd><p>copied to that device</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>Module: self</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Module.double">
<code class="sig-name descname">double</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Module.double" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">double</span></code> datatype.</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Module: self</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.typing_.Module.dump_patches">
<code class="sig-name descname">dump_patches</code><em class="property"> = False</em><a class="headerlink" href="#TorchSnippet.typing_.Module.dump_patches" title="Permalink to this definition">¶</a></dt>
<dd><p>This allows better BC support for <a class="reference internal" href="#TorchSnippet.typing_.Module.load_state_dict" title="TorchSnippet.typing_.Module.load_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a>. In
<a class="reference internal" href="#TorchSnippet.typing_.Module.state_dict" title="TorchSnippet.typing_.Module.state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code></a>, the version number will be saved as in the attribute
<cite>_metadata</cite> of the returned state dict, and thus pickled. <cite>_metadata</cite> is a
dictionary with keys that follow the naming convention of state dict. See
<code class="docutils literal notranslate"><span class="pre">_load_from_state_dict</span></code> on how to use this information in loading.</p>
<p>If new parameters/buffers are added/removed from a module, this number shall
be bumped, and the module’s <cite>_load_from_state_dict</cite> method can compare the
version number and do appropriate changes if the state dict is from before
the change.</p>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Module.eval">
<code class="sig-name descname">eval</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Module.eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the module in evaluation mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. <code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm</span></code>,
etc.</p>
<p>This is equivalent with <code class="xref py py-meth docutils literal notranslate"><span class="pre">self.train(False)</span></code>.</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Module: self</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Module.extra_repr">
<code class="sig-name descname">extra_repr</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Module.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should reimplement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Module.float">
<code class="sig-name descname">float</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Module.float" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to float datatype.</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Module: self</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Module.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">*input</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Module.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <a class="reference internal" href="#TorchSnippet.typing_.Module" title="TorchSnippet.typing_.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Module.half">
<code class="sig-name descname">half</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Module.half" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">half</span></code> datatype.</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Module: self</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Module.load_state_dict">
<code class="sig-name descname">load_state_dict</code><span class="sig-paren">(</span><em class="sig-param">state_dict</em>, <em class="sig-param">strict=True</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Module.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies parameters and buffers from <a class="reference internal" href="#TorchSnippet.typing_.Module.state_dict" title="TorchSnippet.typing_.Module.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> into
this module and its descendants. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">strict</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then
the keys of <a class="reference internal" href="#TorchSnippet.typing_.Module.state_dict" title="TorchSnippet.typing_.Module.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> must exactly match the keys returned
by this module’s <code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code> function.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>state_dict (dict): a dict containing parameters and</dt><dd><p>persistent buffers.</p>
</dd>
<dt>strict (bool, optional): whether to strictly enforce that the keys</dt><dd><p>in <a class="reference internal" href="#TorchSnippet.typing_.Module.state_dict" title="TorchSnippet.typing_.Module.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> match the keys returned by this module’s
<code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code> function. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">NamedTuple</span></code> with <code class="docutils literal notranslate"><span class="pre">missing_keys</span></code> and <code class="docutils literal notranslate"><span class="pre">unexpected_keys</span></code> fields:</dt><dd><ul class="simple">
<li><p><strong>missing_keys</strong> is a list of str containing the missing keys</p></li>
<li><p><strong>unexpected_keys</strong> is a list of str containing the unexpected keys</p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Module.modules">
<code class="sig-name descname">modules</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Module.modules" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over all modules in the network.</p>
<dl class="simple">
<dt>Yields:</dt><dd><p>Module: a module in the network</p>
</dd>
<dt>Note:</dt><dd><p>Duplicate modules are returned only once. In the following
example, <code class="docutils literal notranslate"><span class="pre">l</span></code> will be returned only once.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">modules</span><span class="p">()):</span>
<span class="go">        print(idx, &#39;-&gt;&#39;, m)</span>

<span class="go">0 -&gt; Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
<span class="go">1 -&gt; Linear(in_features=2, out_features=2, bias=True)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Module.named_buffers">
<code class="sig-name descname">named_buffers</code><span class="sig-paren">(</span><em class="sig-param">prefix=''</em>, <em class="sig-param">recurse=True</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Module.named_buffers" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module buffers, yielding both the
name of the buffer as well as the buffer itself.</p>
<dl>
<dt>Args:</dt><dd><p>prefix (str): prefix to prepend to all buffer names.
recurse (bool): if True, then yields buffers of this module</p>
<blockquote>
<div><p>and all submodules. Otherwise, yields only buffers that
are direct members of this module.</p>
</div></blockquote>
</dd>
<dt>Yields:</dt><dd><p>(string, torch.Tensor): Tuple containing the name and buffer</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">buf</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;running_var&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="nb">print</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Module.named_children">
<code class="sig-name descname">named_children</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Module.named_children" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over immediate children modules, yielding both
the name of the module as well as the module itself.</p>
<dl class="simple">
<dt>Yields:</dt><dd><p>(string, Module): Tuple containing a name and child module</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;conv4&#39;</span><span class="p">,</span> <span class="s1">&#39;conv5&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">print</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Module.named_modules">
<code class="sig-name descname">named_modules</code><span class="sig-paren">(</span><em class="sig-param">memo=None</em>, <em class="sig-param">prefix=''</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Module.named_modules" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over all modules in the network, yielding
both the name of the module as well as the module itself.</p>
<dl class="simple">
<dt>Yields:</dt><dd><p>(string, Module): Tuple of name and module</p>
</dd>
<dt>Note:</dt><dd><p>Duplicate modules are returned only once. In the following
example, <code class="docutils literal notranslate"><span class="pre">l</span></code> will be returned only once.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">named_modules</span><span class="p">()):</span>
<span class="go">        print(idx, &#39;-&gt;&#39;, m)</span>

<span class="go">0 -&gt; (&#39;&#39;, Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">))</span>
<span class="go">1 -&gt; (&#39;0&#39;, Linear(in_features=2, out_features=2, bias=True))</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Module.named_parameters">
<code class="sig-name descname">named_parameters</code><span class="sig-paren">(</span><em class="sig-param">prefix=''</em>, <em class="sig-param">recurse=True</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Module.named_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module parameters, yielding both the
name of the parameter as well as the parameter itself.</p>
<dl>
<dt>Args:</dt><dd><p>prefix (str): prefix to prepend to all parameter names.
recurse (bool): if True, then yields parameters of this module</p>
<blockquote>
<div><p>and all submodules. Otherwise, yields only parameters that
are direct members of this module.</p>
</div></blockquote>
</dd>
<dt>Yields:</dt><dd><p>(string, Parameter): Tuple containing the name and parameter</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="nb">print</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Module.parameters">
<code class="sig-name descname">parameters</code><span class="sig-paren">(</span><em class="sig-param">recurse=True</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Module.parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module parameters.</p>
<p>This is typically passed to an optimizer.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>recurse (bool): if True, then yields parameters of this module</dt><dd><p>and all submodules. Otherwise, yields only parameters that
are direct members of this module.</p>
</dd>
</dl>
</dd>
<dt>Yields:</dt><dd><p>Parameter: module parameter</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="p">),</span> <span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">&lt;class &#39;torch.FloatTensor&#39;&gt; (20L,)</span>
<span class="go">&lt;class &#39;torch.FloatTensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Module.register_backward_hook">
<code class="sig-name descname">register_backward_hook</code><span class="sig-paren">(</span><em class="sig-param">hook</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Module.register_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a backward hook on the module.</p>
<p>The hook will be called every time the gradients with respect to module
inputs are computed. The hook should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span> <span class="ow">or</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_output</span></code> may be tuples if the
module has multiple inputs or outputs. The hook should not modify its
arguments, but it can optionally return a new gradient with respect to
input that will be used in place of <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> in subsequent
computations.</p>
<dl class="simple">
<dt>Returns:</dt><dd><dl class="simple">
<dt><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code>:</dt><dd><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The current implementation will not have the presented behavior
for complex <a class="reference internal" href="#TorchSnippet.typing_.Module" title="TorchSnippet.typing_.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> that perform many operations.
In some failure cases, <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_output</span></code> will only
contain the gradients for a subset of the inputs and outputs.
For such <a class="reference internal" href="#TorchSnippet.typing_.Module" title="TorchSnippet.typing_.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a>, you should use <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.register_hook()</span></code>
directly on a specific input or output to get the required gradients.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Module.register_buffer">
<code class="sig-name descname">register_buffer</code><span class="sig-paren">(</span><em class="sig-param">name</em>, <em class="sig-param">tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Module.register_buffer" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a persistent buffer to the module.</p>
<p>This is typically used to register a buffer that should not to be
considered a model parameter. For example, BatchNorm’s <code class="docutils literal notranslate"><span class="pre">running_mean</span></code>
is not a parameter, but is part of the persistent state.</p>
<p>Buffers can be accessed as attributes using given names.</p>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>name (string): name of the buffer. The buffer can be accessed</dt><dd><p>from this module using the given name</p>
</dd>
</dl>
<p>tensor (Tensor): buffer to be registered.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;running_mean&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_features</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Module.register_forward_hook">
<code class="sig-name descname">register_forward_hook</code><span class="sig-paren">(</span><em class="sig-param">hook</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Module.register_forward_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a forward hook on the module.</p>
<p>The hook will be called every time after <a class="reference internal" href="#TorchSnippet.typing_.Module.forward" title="TorchSnippet.typing_.Module.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> has computed an output.
It should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">modified</span> <span class="n">output</span>
</pre></div>
</div>
<p>The hook can modify the output. It can modify the input inplace but
it will not have effect on forward since this is called after
<a class="reference internal" href="#TorchSnippet.typing_.Module.forward" title="TorchSnippet.typing_.Module.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> is called.</p>
<dl class="simple">
<dt>Returns:</dt><dd><dl class="simple">
<dt><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code>:</dt><dd><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Module.register_forward_pre_hook">
<code class="sig-name descname">register_forward_pre_hook</code><span class="sig-paren">(</span><em class="sig-param">hook</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Module.register_forward_pre_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a forward pre-hook on the module.</p>
<p>The hook will be called every time before <a class="reference internal" href="#TorchSnippet.typing_.Module.forward" title="TorchSnippet.typing_.Module.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> is invoked.
It should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">modified</span> <span class="nb">input</span>
</pre></div>
</div>
<p>The hook can modify the input. User can either return a tuple or a
single modified value in the hook. We will wrap the value into a tuple
if a single value is returned(unless that value is already a tuple).</p>
<dl class="simple">
<dt>Returns:</dt><dd><dl class="simple">
<dt><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code>:</dt><dd><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Module.register_parameter">
<code class="sig-name descname">register_parameter</code><span class="sig-paren">(</span><em class="sig-param">name</em>, <em class="sig-param">param</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Module.register_parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a parameter to the module.</p>
<p>The parameter can be accessed as an attribute using given name.</p>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>name (string): name of the parameter. The parameter can be accessed</dt><dd><p>from this module using the given name</p>
</dd>
</dl>
<p>param (Parameter): parameter to be added to the module.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Module.requires_grad_">
<code class="sig-name descname">requires_grad_</code><span class="sig-paren">(</span><em class="sig-param">requires_grad=True</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Module.requires_grad_" title="Permalink to this definition">¶</a></dt>
<dd><p>Change if autograd should record operations on parameters in this
module.</p>
<p>This method sets the parameters’ <code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad</span></code> attributes
in-place.</p>
<p>This method is helpful for freezing part of the module for finetuning
or training parts of a model individually (e.g., GAN training).</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>requires_grad (bool): whether autograd should record operations on</dt><dd><p>parameters in this module. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>Module: self</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Module.share_memory">
<code class="sig-name descname">share_memory</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Module.share_memory" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Module.state_dict">
<code class="sig-name descname">state_dict</code><span class="sig-paren">(</span><em class="sig-param">destination=None</em>, <em class="sig-param">prefix=''</em>, <em class="sig-param">keep_vars=False</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Module.state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a dictionary containing a whole state of the module.</p>
<p>Both parameters and persistent buffers (e.g. running averages) are
included. Keys are corresponding parameter and buffer names.</p>
<dl class="simple">
<dt>Returns:</dt><dd><dl class="simple">
<dt>dict:</dt><dd><p>a dictionary containing a whole state of the module</p>
</dd>
</dl>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
<span class="go">[&#39;bias&#39;, &#39;weight&#39;]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Module.to">
<code class="sig-name descname">to</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Module.to" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves and/or casts the parameters and buffers.</p>
<p>This can be called as</p>
<dl class="function">
<dt>
<code class="sig-name descname">to</code><span class="sig-paren">(</span><em class="sig-param">device=None</em>, <em class="sig-param">dtype=None</em>, <em class="sig-param">non_blocking=False</em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<dl class="function">
<dt>
<code class="sig-name descname">to</code><span class="sig-paren">(</span><em class="sig-param">dtype</em>, <em class="sig-param">non_blocking=False</em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<dl class="function">
<dt>
<code class="sig-name descname">to</code><span class="sig-paren">(</span><em class="sig-param">tensor</em>, <em class="sig-param">non_blocking=False</em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<p>Its signature is similar to <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.to()</span></code>, but only accepts
floating point desired <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code> s. In addition, this method will
only cast the floating point parameters and buffers to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code>
(if given). The integral parameters and buffers will be moved
<code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code>, if that is given, but with dtypes unchanged. When
<code class="xref py py-attr docutils literal notranslate"><span class="pre">non_blocking</span></code> is set, it tries to convert/move asynchronously
with respect to the host if possible, e.g., moving CPU Tensors with
pinned memory to CUDA devices.</p>
<p>See below for examples.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>device (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code>): the desired device of the parameters</dt><dd><p>and buffers in this module</p>
</dd>
<dt>dtype (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code>): the desired floating point type of</dt><dd><p>the floating point parameters and buffers in this module</p>
</dd>
<dt>tensor (torch.Tensor): Tensor whose dtype and device are the desired</dt><dd><p>dtype and device for all parameters and buffers in this module</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>Module: self</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.1913, -0.3420],</span>
<span class="go">        [-0.5113, -0.2325]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.1913, -0.3420],</span>
<span class="go">        [-0.5113, -0.2325]], dtype=torch.float64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gpu1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">gpu1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">half</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.1914, -0.3420],</span>
<span class="go">        [-0.5112, -0.2324]], dtype=torch.float16, device=&#39;cuda:1&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cpu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cpu</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.1914, -0.3420],</span>
<span class="go">        [-0.5112, -0.2324]], dtype=torch.float16)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Module.train">
<code class="sig-name descname">train</code><span class="sig-paren">(</span><em class="sig-param">mode=True</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Module.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the module in training mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. <code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm</span></code>,
etc.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>mode (bool): whether to set training mode (<code class="docutils literal notranslate"><span class="pre">True</span></code>) or evaluation</dt><dd><p>mode (<code class="docutils literal notranslate"><span class="pre">False</span></code>). Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>Module: self</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Module.type">
<code class="sig-name descname">type</code><span class="sig-paren">(</span><em class="sig-param">dst_type</em><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Module.type" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all parameters and buffers to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dst_type</span></code>.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>dst_type (type or string): the desired type</p>
</dd>
<dt>Returns:</dt><dd><p>Module: self</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="TorchSnippet.typing_.Module.zero_grad">
<code class="sig-name descname">zero_grad</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#TorchSnippet.typing_.Module.zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets gradients of all model parameters to zero.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="TorchSnippet.typing_.WeightNormMode">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.typing_.</code><code class="sig-name descname">WeightNormMode</code><a class="headerlink" href="#TorchSnippet.typing_.WeightNormMode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">enum.Enum</span></code></p>
<p>An enumeration.</p>
<dl class="attribute">
<dt id="TorchSnippet.typing_.WeightNormMode.FULL">
<code class="sig-name descname">FULL</code><em class="property"> = 'full'</em><a class="headerlink" href="#TorchSnippet.typing_.WeightNormMode.FULL" title="Permalink to this definition">¶</a></dt>
<dd><p>Full weight norm, i.e., using <cite>g * v / norm(v)</cite> as the new weight.</p>
</dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.typing_.WeightNormMode.NONE">
<code class="sig-name descname">NONE</code><em class="property"> = 'none'</em><a class="headerlink" href="#TorchSnippet.typing_.WeightNormMode.NONE" title="Permalink to this definition">¶</a></dt>
<dd><p>No weight norm.</p>
</dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.typing_.WeightNormMode.NO_SCALE">
<code class="sig-name descname">NO_SCALE</code><em class="property"> = 'no_scale'</em><a class="headerlink" href="#TorchSnippet.typing_.WeightNormMode.NO_SCALE" title="Permalink to this definition">¶</a></dt>
<dd><p>Weight norm without scale, i.e., using <cite>v / norm(v)</cite></p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="TorchSnippet.typing_.PaddingMode">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.typing_.</code><code class="sig-name descname">PaddingMode</code><a class="headerlink" href="#TorchSnippet.typing_.PaddingMode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">enum.Enum</span></code></p>
<p>An enumeration.</p>
<dl class="attribute">
<dt id="TorchSnippet.typing_.PaddingMode.DEFAULT">
<code class="sig-name descname">DEFAULT</code><em class="property"> = 'none'</em><a class="headerlink" href="#TorchSnippet.typing_.PaddingMode.DEFAULT" title="Permalink to this definition">¶</a></dt>
<dd><p>The default padding mode is “none”.</p>
</dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.typing_.PaddingMode.FULL">
<code class="sig-name descname">FULL</code><em class="property"> = 'full'</em><a class="headerlink" href="#TorchSnippet.typing_.PaddingMode.FULL" title="Permalink to this definition">¶</a></dt>
<dd><p>Padding size is <cite>kernel_size - 1</cite> at each side.</p>
</dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.typing_.PaddingMode.HALF">
<code class="sig-name descname">HALF</code><em class="property"> = 'half'</em><a class="headerlink" href="#TorchSnippet.typing_.PaddingMode.HALF" title="Permalink to this definition">¶</a></dt>
<dd><p>Padding size is half the size of the kernel at each side, i.e.,
<cite>left_padding = right_padding = kernel_size // 2</cite>.</p>
<p>The kernel size for each spatial dimension must be odd, in order to
use this mode.  Also, when <cite>stride</cite> == 1, this mode will cause the
output shape to be the same as the input shape.</p>
</dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.typing_.PaddingMode.NONE">
<code class="sig-name descname">NONE</code><em class="property"> = 'none'</em><a class="headerlink" href="#TorchSnippet.typing_.PaddingMode.NONE" title="Permalink to this definition">¶</a></dt>
<dd><p>No padding, i.e., <cite>left_padding = right_padding = 0</cite>.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="TorchSnippet.typing_.ActNormScaleType">
<em class="property">class </em><code class="sig-prename descclassname">TorchSnippet.typing_.</code><code class="sig-name descname">ActNormScaleType</code><a class="headerlink" href="#TorchSnippet.typing_.ActNormScaleType" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">enum.Enum</span></code></p>
<p>Scale type of <code class="xref py py-class docutils literal notranslate"><span class="pre">tk.layers.ActNorm</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">tk.flows.ActNorm</span></code>.</p>
<dl class="attribute">
<dt id="TorchSnippet.typing_.ActNormScaleType.EXP">
<code class="sig-name descname">EXP</code><em class="property"> = 'exp'</em><a class="headerlink" href="#TorchSnippet.typing_.ActNormScaleType.EXP" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="TorchSnippet.typing_.ActNormScaleType.LINEAR">
<code class="sig-name descname">LINEAR</code><em class="property"> = 'linear'</em><a class="headerlink" href="#TorchSnippet.typing_.ActNormScaleType.LINEAR" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-TorchSnippet">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-TorchSnippet" title="Permalink to this headline">¶</a></h2>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="TorchSnippet.Flow.html" class="btn btn-neutral float-right" title="TorchSnippet.Flow package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="modules.html" class="btn btn-neutral float-left" title="TorchSnippet" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Tijin Yan

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>